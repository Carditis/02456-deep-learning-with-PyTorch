{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAva8TnYFtFu"
   },
   "source": [
    "# Contents and why we need this lab\n",
    "\n",
    "This lab is about implementing neural networks yourself before we start using other frameworks which hide some of the computation from you. It builds on the first lab where you derived the equations for neural network forward and backward propagation and gradient descent parameter updates. \n",
    "\n",
    "All the frameworks for deep learning you will meet from now on uses automatic differentiation (autodiff) so you don't have to code the backward step yourself. In this version of this lab you will develop your own autodif implementation. We also have a [version](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/2_Feedforward_NumPy/2.1-FNN-NumPy.ipynb) of this lab where you have to code the backward pass explicitly in Numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCa7HzwpFtFy"
   },
   "source": [
    "# External sources of information\n",
    "\n",
    "1. Jupyter notebook. You can find more information about Jupyter notebooks [here](https://jupyter.org/). It will come as part of the [Anaconda](https://www.anaconda.com/) Python installation. \n",
    "2. [NumPy](https://numpy.org/). Part of Anaconda distribution. If you already know how to program most things about Python and NumPy can be found through Google search.\n",
    "3. [Nanograd](https://github.com/rasmusbergpalm/nanograd) is a minimalistic version of autodiff developed by Rasmus Berg Palm that we use for our framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SjiIp-TFtF0"
   },
   "source": [
    "# This notebook will follow the next steps:\n",
    "\n",
    "1. Nanograd automatic differentiation framework\n",
    "2. Finite difference method\n",
    "3. Data generation\n",
    "4. Defining and initializing the network\n",
    "5. Forward pass\n",
    "6. Training loop \n",
    "7. Testing your model\n",
    "8. Further extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyXeAA-HuT7s"
   },
   "source": [
    "# Nanograd automatic differention framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6UWKCLKubgA"
   },
   "source": [
    "The [Nanograd](https://github.com/rasmusbergpalm/nanograd) framework defines a class Var which both holds a value and gradient value that we can use to store the intermediate values when we apply the chain rule of differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Jd4CoEBNzNWS"
   },
   "outputs": [],
   "source": [
    "# Copy and pasted from https://github.com/rasmusbergpalm/nanograd/blob/main/nanograd.py\n",
    "\n",
    "from math import exp, log\n",
    "\n",
    "class Var:\n",
    "    \"\"\"\n",
    "    A variable which holds a float and enables gradient computations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, val: float, grad_fn=lambda: []):\n",
    "        assert type(val) == float\n",
    "        self.v = val\n",
    "        self.grad_fn = grad_fn\n",
    "        self.grad = 0.0\n",
    "\n",
    "    def backprop(self, bp):\n",
    "        self.grad += bp\n",
    "        for (input, grad) in self.grad_fn():\n",
    "            input.backprop(grad * bp)\n",
    "\n",
    "    def backward(self):\n",
    "        self.backprop(1.0)\n",
    "\n",
    "    def __add__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return Var(self.v + other.v, lambda: [(self, 1.0), (other, 1.0)])\n",
    "\n",
    "    def __mul__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return Var(self.v * other.v, lambda: [(self, other.v), (other, self.v)])\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        assert type(power) in {float, int}, \"power must be float or int\"\n",
    "        return Var(self.v ** power, lambda: [(self, power * self.v ** (power - 1))])\n",
    "\n",
    "    def __neg__(self: 'Var') -> 'Var':\n",
    "        return Var(-1.0) * self\n",
    "\n",
    "    def __sub__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return self + (-other)\n",
    "\n",
    "    def __truediv__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return self * other ** -1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Var(v=%.4f, grad=%.4f)\" % (self.v, self.grad)\n",
    "\n",
    "    def relu(self):\n",
    "        return Var(self.v if self.v > 0.0 else 0.0, lambda: [(self, 1.0 if self.v > 0.0 else 0.0)])\n",
    "    \n",
    "    def identity(self):\n",
    "        return Var(self.v, lambda: [(self,self.v)])\n",
    "    \n",
    "    def tanh(self):\n",
    "        return Var((math.exp(self.v)-math.exp(-self.v))/(math.exp(self.v)+math.exp(-self.v)), lambda: [(self,1-(((math.exp(self.v)-math.exp(-self.v))/(math.exp(self.v)+math.exp(-self.v)))**2))])\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        return Var(1.0/1.0+exp(-self.v) , lambda: [(self, (1.0/1.0+exp(-self.v))/(1-(1.0/1.0+exp(-self.v))))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDX67D6jzcte"
   },
   "source": [
    "A few examples illustrate how we can use this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testi test for understanding :)\n",
    "\n",
    "#a = Var(2.0)\n",
    "#b = Var(2.0)\n",
    "#c = Var(2.0)\n",
    "#print(a)\n",
    "#print(b)\n",
    "#print(c)\n",
    "#f = a*b\n",
    "#print(f)\n",
    "#f.backward()\n",
    "#print(a.v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xk6PeLc3zwPT",
    "outputId": "47e431b2-07ba-4cb1-ea21-997769641c67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var(v=3.0000, grad=5.0000)\n",
      "Var(v=5.0000, grad=3.0000)\n",
      "Var(v=15.0000, grad=1.0000)\n"
     ]
    }
   ],
   "source": [
    "a = Var(3.0)\n",
    "b = Var(5.0)\n",
    "f = a * b\n",
    "\n",
    "f.backward()\n",
    "\n",
    "for i in [a, b, f]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JmKhYgsY0g_o",
    "outputId": "06c1b1df-c33c-40d3-922a-624612a591c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var(v=3.0000, grad=14.0000)\n",
      "Var(v=5.0000, grad=3.0000)\n",
      "Var(v=15.0000, grad=1.0000)\n",
      "Var(v=9.0000, grad=3.0000)\n",
      "Var(v=27.0000, grad=1.0000)\n",
      "Var(v=42.0000, grad=1.0000)\n"
     ]
    }
   ],
   "source": [
    "a = Var(3.0)\n",
    "b = Var(5.0)\n",
    "c = a * b\n",
    "d = Var(9.0)\n",
    "e = a * d\n",
    "f = c + e\n",
    "\n",
    "f.backward()\n",
    "\n",
    "for i in [a, b, c, d, e, f]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe3B6uEH140p"
   },
   "source": [
    "## Exercise a) What is being calculated?\n",
    "\n",
    "Explain briefly the output of the code? What is the expression we differentiate and with respect to what variables?\n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "Multiplication and addition will create a new instance, that have the value of dertermined by the opperator and a list of the included factor (instances used (a and b)). If the new instance f is creates as the product of a and b, will backprob first opdate a's grad by differentiating f with respect to a and then do the same for b.\n",
    "\n",
    "This can be expanded as in the other exsample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8_Q0t2I3Ruj"
   },
   "source": [
    "## Exercise b) How does the backward function work?\n",
    "\n",
    "You need to understand how the backward function calculates the gradients. We can use the two examples above to help with that.\n",
    "\n",
    "Go through the following four steps and answer the questions on the way:\n",
    "\n",
    "1. We represent the two expressions as graphs as shown below. Fill in the missing expressions for the different derivatives.\n",
    "\n",
    "2. In the remainder consider the first expression. Make a schematic of the data structure which is generated when we define the expression for f. \n",
    "\n",
    "3. Then execute the backward function by hand to convince yourself that it indeed calculates the gradients with respect to the variables. \n",
    "\n",
    "4. Write down the sequence of calls to backprop.\n",
    "\n",
    "\n",
    "Answer:\n",
    "\n",
    "$$\n",
    " a.grad =   \\frac{df}{dv_{-1}} = \\frac{df}{dv_1} \\cdot \\frac{dv_1}{dv_{-1}} = 1 \\cdot \\frac{d(a.v\\cdot b.v)}{da.v} = b.v\n",
    "$$\n",
    "$$\n",
    " b.grad =   \\frac{df}{dv_{0}} = \\frac{df}{dv_1} \\cdot \\frac{dv_1}{dv_{0}} = 1 \\cdot \\frac{d(a.v\\cdot b.v)}{da.v} = a.v\n",
    "$$\n",
    "\n",
    "This shows the chain rule being utelized for calculating the derivative for a and b. The v's represent a link each in the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "idGr71jYXl26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG@graphviz.backend.execute] run [WindowsPath('dot'), '-V']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('0.20.1', (2, 50, 0))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import graphviz\n",
    "\n",
    "logging.basicConfig(format='[%(levelname)s@%(name)s] %(message)s', level=logging.DEBUG)\n",
    "\n",
    "graphviz.__version__, graphviz.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152
    },
    "id": "KPe30Q2QXzeG",
    "outputId": "7fa002cd-a018-4dbb-ddf1-28ed5e99ee19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG@graphviz.backend.execute] run [WindowsPath('dot'), '-Kdot', '-Tsvg']\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Title: first expression Pages: 1 -->\n",
       "<svg width=\"159pt\" height=\"98pt\"\n",
       " viewBox=\"0.00 0.00 159.00 98.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 94)\">\n",
       "<title>first expression</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-94 155,-94 155,4 -4,4\"/>\n",
       "<!-- a -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>a</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"18\" cy=\"-72\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"18\" y=\"-68.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">a</text>\n",
       "</g>\n",
       "<!-- f -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>f</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"133\" cy=\"-45\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"133\" y=\"-41.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f</text>\n",
       "</g>\n",
       "<!-- a&#45;&gt;f -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>a&#45;&gt;f</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M35.67,-68.02C54.03,-63.63 83.81,-56.51 105.4,-51.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"106.35,-54.73 115.26,-49 104.72,-47.92 106.35,-54.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"75.5\" y=\"-66.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">df/da=b</text>\n",
       "</g>\n",
       "<!-- b -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>b</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"18\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"18\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">b</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;f -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>b&#45;&gt;f</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M35.85,-20.94C51.73,-23.83 76.12,-28.59 97,-34 99.89,-34.75 102.9,-35.59 105.88,-36.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"105.13,-39.9 115.72,-39.5 107.19,-33.21 105.13,-39.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"75.5\" y=\"-37.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">df/db=a</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x29cdf2fa5b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1 = graphviz.Digraph('first expression', filename='fsm.gv')\n",
    "\n",
    "e1.attr(rankdir='LR', size='8,5')\n",
    "\n",
    "e1.attr('node', shape='circle')\n",
    "e1.edge('a', 'f', label='df/da=b')\n",
    "e1.edge('b', 'f', label='df/db=a')\n",
    "\n",
    "e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "0nittR-mZFeX",
    "outputId": "fa3656a3-732c-4abe-8084-98a492b0d6be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG@graphviz.backend.execute] run [WindowsPath('dot'), '-Kdot', '-Tsvg']\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Title: second expression Pages: 1 -->\n",
       "<svg width=\"341pt\" height=\"158pt\"\n",
       " viewBox=\"0.00 0.00 341.00 158.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 154)\">\n",
       "<title>second expression</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-154 337,-154 337,4 -4,4\"/>\n",
       "<!-- a -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>a</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"18\" cy=\"-75\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"18\" y=\"-71.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">a</text>\n",
       "</g>\n",
       "<!-- c -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>c</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"200\" cy=\"-102\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"200\" y=\"-98.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">c</text>\n",
       "</g>\n",
       "<!-- a&#45;&gt;c -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>a&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M36.02,-77.01C63.16,-80.29 117.89,-87.22 164,-95 166.71,-95.46 169.54,-95.97 172.35,-96.5\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"171.82,-99.96 182.3,-98.45 173.16,-93.09 171.82,-99.96\"/>\n",
       "<text text-anchor=\"middle\" x=\"109\" y=\"-98.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dc/da=df/e+df/dc=b</text>\n",
       "</g>\n",
       "<!-- e -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>e</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"200\" cy=\"-48\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"200\" y=\"-44.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">e</text>\n",
       "</g>\n",
       "<!-- a&#45;&gt;e -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>a&#45;&gt;e</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M34.31,-67.17C40.3,-64.45 47.33,-61.66 54,-60 93.99,-50.05 141.91,-47.91 171.4,-47.67\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"171.75,-51.17 181.75,-47.65 171.74,-44.17 171.75,-51.17\"/>\n",
       "<text text-anchor=\"middle\" x=\"109\" y=\"-63.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">de/da=b</text>\n",
       "</g>\n",
       "<!-- f -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>f</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"315\" cy=\"-75\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"315\" y=\"-71.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">f</text>\n",
       "</g>\n",
       "<!-- c&#45;&gt;f -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>c&#45;&gt;f</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M217.67,-98.02C236.03,-93.63 265.81,-86.51 287.4,-81.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"288.35,-84.73 297.26,-79 286.72,-77.92 288.35,-84.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"257.5\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">df/dc=1</text>\n",
       "</g>\n",
       "<!-- b -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>b</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"18\" cy=\"-132\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"18\" y=\"-128.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">b</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;c -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>b&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M36.03,-129.83C63.2,-126.28 117.97,-118.74 164,-110 166.72,-109.48 169.55,-108.91 172.37,-108.3\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"173.32,-111.68 182.32,-106.08 171.8,-104.84 173.32,-111.68\"/>\n",
       "<text text-anchor=\"middle\" x=\"109\" y=\"-130.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">dc/db=a</text>\n",
       "</g>\n",
       "<!-- e&#45;&gt;f -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>e&#45;&gt;f</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M217.85,-50.94C233.73,-53.83 258.12,-58.59 279,-64 281.89,-64.75 284.9,-65.59 287.88,-66.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"287.13,-69.9 297.72,-69.5 289.19,-63.21 287.13,-69.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"257.5\" y=\"-67.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">df/de=1</text>\n",
       "</g>\n",
       "<!-- d -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>d</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"18\" cy=\"-18\" rx=\"18\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"18\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d</text>\n",
       "</g>\n",
       "<!-- d&#45;&gt;e -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>d&#45;&gt;e</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M35.96,-16.76C63.49,-15.27 119.32,-14.47 164,-28 167.96,-29.2 171.95,-30.91 175.75,-32.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"174.26,-36 184.69,-37.87 177.7,-29.91 174.26,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"109\" y=\"-31.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">de/dd=a</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x29cdf2fafd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2 = graphviz.Digraph('second expression', filename='fsm.gv')\n",
    "\n",
    "e2.attr(rankdir='LR', size='8,5')\n",
    "\n",
    "e2.attr('node', shape='circle')\n",
    "e2.edge('a', 'c', label='dc/da=df/e+df/dc=b')\n",
    "e2.edge('b', 'c', label='dc/db=a')\n",
    "e2.edge('a', 'e', label='de/da=b')\n",
    "e2.edge('d', 'e', label='de/dd=a')\n",
    "e2.edge('c', 'f', label='df/dc=1')\n",
    "e2.edge('e', 'f', label='df/de=1')\n",
    "\n",
    "e2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5oi21W4gpeM"
   },
   "source": [
    "## Exercise c) What happens if we run backward again?\n",
    "\n",
    "Try to execute the code below. Explain what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCtpJyr-gyX1",
    "outputId": "d014bcfa-c9ae-49c3-d268-91cc6ca94ea5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var(v=3.0000, grad=28.0000)\n",
      "Var(v=5.0000, grad=6.0000)\n",
      "Var(v=15.0000, grad=2.0000)\n",
      "Var(v=9.0000, grad=6.0000)\n",
      "Var(v=27.0000, grad=2.0000)\n",
      "Var(v=42.0000, grad=2.0000)\n"
     ]
    }
   ],
   "source": [
    "f.backward()\n",
    "\n",
    "for v in [a, b, c, d, e, f]:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "We keep adding the same amount to each gradient as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8bPVq2VhsP-"
   },
   "source": [
    "## Exercise d) Zero gradient\n",
    "\n",
    "We can zero the gradient by backpropagating a -1.0 as is shown in the example below. (If you have run backward multiple time then you also have to run the cell below an equal amount of times.) Explain what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnyPDQx9lJe0",
    "outputId": "7a125fdc-60c4-4340-a580-8b82aea5b0db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var(v=2.0000, grad=0.0000)\n",
      "Var(v=5.0000, grad=6.0000)\n",
      "Var(v=15.0000, grad=2.0000)\n",
      "Var(v=9.0000, grad=6.0000)\n",
      "Var(v=27.0000, grad=2.0000)\n",
      "Var(v=42.0000, grad=2.0000)\n",
      "Var(v=2.0000, grad=0.0000)\n",
      "Var(v=5.0000, grad=3.0000)\n",
      "Var(v=15.0000, grad=1.0000)\n",
      "Var(v=9.0000, grad=3.0000)\n",
      "Var(v=27.0000, grad=1.0000)\n",
      "Var(v=42.0000, grad=1.0000)\n"
     ]
    }
   ],
   "source": [
    "a = Var(2.0)\n",
    "\n",
    "for v in [a, b, c, d, e, f]:\n",
    "    print(v)\n",
    "\n",
    "f.backprop(-1.0)\n",
    "\n",
    "for v in [a, b, c, d, e, f]:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "1 or -1 will work as a direction for the steps taken. This can therefore be seen as 1 being \"a step forward\" and -1 being \"a step backwards\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4057_ljNvWB"
   },
   "source": [
    "## Exercise e) Test correctness of derivatives with the finite difference method\n",
    "\n",
    "Write a small function that uses [the finite difference method](https://en.wikipedia.org/wiki/Finite_difference_method) to numerically test that backpropation implementation is working. In short we will use\n",
    "$$\n",
    "\\frac{\\partial f(a)}{\\partial a} \\approx \\frac{f(a+da)-f(a)}{da}\n",
    "$$\n",
    "for $da \\ll 1$.\n",
    "\n",
    "As an example, we could approximate the derivative of the function $f(a)=a^2$ in e.g. the value $a=4$ using the finite difference method. This amounts to inserting the relevant values and approximating the gradient $f'(4)$ with the fraction above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9TGil92lSXDN",
    "outputId": "7ef5489b-b525-4132-ab08-0b1109c07f4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var(v=3.0000, grad=5.0000)\n",
      "Var(v=5.0000, grad=3.0000)\n",
      "Var(v=15.0000, grad=1.0000)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-213c646026ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfinite_difference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinite_difference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-213c646026ec>\u001b[0m in \u001b[0;36mfinite_difference\u001b[1;34m(da)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \"\"\"\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mfa_da\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mda\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m           \u001b[1;31m# <- Insert correct expression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mfa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv\u001b[0m              \u001b[1;31m# <- Insert correct expression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "# f function - try to change the code to test other types of functions as well (such as different polynomials etc.)\n",
    "def f_function(a):\n",
    "    a = Var(a)\n",
    "    b = Var(5.0)\n",
    "    f = a * b\n",
    "    f.backward()\n",
    "    return a,b,f\n",
    "\n",
    "for v in f_function(3.0):\n",
    "  print(v)\n",
    "\n",
    "# Insert your finite difference code here\n",
    "def finite_difference(da=1e-10):\n",
    "    \"\"\"\n",
    "    This function compute the finite difference between\n",
    "    \n",
    "    Input:\n",
    "    da:          The finite difference                           (float)\n",
    "    \n",
    "    Output:\n",
    "    finite_difference: numerical approximation to the derivative (float) \n",
    "    \"\"\"\n",
    "\n",
    "    fa_da = f.v + (da * b.v)           # <- Insert correct expression\n",
    "    fa = f.v              # <- Insert correct expression\n",
    "\n",
    "    finite_difference = (fa_da - fa) / da\n",
    "    \n",
    "    return finite_difference\n",
    "\n",
    "print(finite_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pZar5RKaUkg"
   },
   "source": [
    "# Create an artificial dataset to play with\n",
    "\n",
    "We create a non-linear 1d regression task. The generator supports various noise levels and it creates train, validation and test sets. You can modify it yourself if you want more or less challenging tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Y6yfMAQ8aduj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4YabfD43ajNh"
   },
   "outputs": [],
   "source": [
    "def data_generator(noise=0.1, n_samples=300, D1=True):\n",
    "    # Create covariates and response variable\n",
    "    if D1:\n",
    "        X = np.linspace(-3, 3, num=n_samples).reshape(-1,1) # 1-D\n",
    "        np.random.shuffle(X)\n",
    "        y = np.random.normal((0.5*np.sin(X[:,0]*3) + X[:,0]), noise) # 1-D with trend\n",
    "    else:\n",
    "        X = np.random.multivariate_normal(np.zeros(3), noise*np.eye(3), size = n_samples) # 3-D\n",
    "        np.random.shuffle(X)    \n",
    "        y = np.sin(X[:,0]) - 5*(X[:,1]**2) + 0.5*X[:,2] # 3-D\n",
    "\n",
    "    # Stack them together vertically to split data set\n",
    "    data_set = np.vstack((X.T,y)).T\n",
    "    \n",
    "    train, validation, test = np.split(data_set, [int(0.35*n_samples), int(0.7*n_samples)], axis=0)\n",
    "    \n",
    "    # Standardization of the data, remember we do the standardization with the training set mean and standard deviation\n",
    "    train_mu = np.mean(train, axis=0)\n",
    "    train_sigma = np.std(train, axis=0)\n",
    "    \n",
    "    train = (train-train_mu)/train_sigma\n",
    "    validation = (validation-train_mu)/train_sigma\n",
    "    test = (test-train_mu)/train_sigma\n",
    "    \n",
    "    x_train, x_validation, x_test = train[:,:-1], validation[:,:-1], test[:,:-1]\n",
    "    y_train, y_validation, y_test = train[:,-1], validation[:,-1], test[:,-1]\n",
    "\n",
    "    return x_train, y_train,  x_validation, y_validation, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "u1oDngHLapIz"
   },
   "outputs": [],
   "source": [
    "D1 = True\n",
    "x_train, y_train,  x_validation, y_validation, x_test, y_test = data_generator(noise=0.5, D1=D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "Ysfa3FsBavlm",
    "outputId": "399e5382-ae7d-48f6-9774-7ea4c73e7d95"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if D1:\n",
    "    plt.scatter(x_train[:,0], y_train);\n",
    "    plt.scatter(x_validation[:,0], y_validation);\n",
    "    plt.scatter(x_test[:,0], y_test);\n",
    "else:\n",
    "    plt.scatter(x_train[:,1], y_train);\n",
    "    plt.scatter(x_validation[:,1], y_validation);\n",
    "    plt.scatter(x_test[:,1], y_test);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zac2HHNlgbpm"
   },
   "outputs": [],
   "source": [
    "# convert from nparray to Var\n",
    "def nparray_to_Var(x):\n",
    "  if x.ndim==1:\n",
    "    y = [[Var(float(x[i]))] for i in range(x.shape[0])] # always work with list of list\n",
    "  else:\n",
    "    y = [[Var(float(x[i,j])) for j in range(x.shape[1])] for i in range(x.shape[0])]\n",
    "  return y\n",
    "   \n",
    "x_train = nparray_to_Var(x_train)\n",
    "y_train = nparray_to_Var(y_train)\n",
    "x_validation = nparray_to_Var(x_validation)\n",
    "y_validation = nparray_to_Var(y_validation)\n",
    "x_test = nparray_to_Var(x_test)\n",
    "y_test = nparray_to_Var(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbjrqcpVFtGe"
   },
   "source": [
    "# Defining and initializing the network\n",
    "\n",
    "The steps to create a feed forward neural network are the following:\n",
    "\n",
    "1. **Number of hidden layer and hidden units**. We have to define the number of hidden units in each layer. The number of features in X and the output dimensionality (the size of Y) are given but the numbers in between are set by the researcher. Remember that for each unit in each layer beside in the input has a bias term.\n",
    "2. **Activation functions** for each hidden layer. Each hidden layer in your list must have an activation function (it can also be the linear activation which is equivalent to identity function). The power of neural networks comes from non-linear activation functions that learn representations (features) from the data allowing us to learn from it. \n",
    "3. **Parameter initialization**. We will initialize the weights to have random values. This is done in practice by drawing pseudo random numbers from a Gaussian or uniform distribution. It turns out that for deeper models we have to be careful about how we scale the random numbers. This will be the topic of the exercise below. For now we will just use unit variance Gaussians.  \n",
    "\n",
    "In order to make life easier for ourselves we define a DenseLayer class that takes care of initialization and the forward pass. We can also extend it later with print and advanced initialization capabilities. For the latter we have introduced a Initializer class.\n",
    "\n",
    "Note that we use Sequence in the code below. A Sequence is an ordered list. This means the order we insert and access items are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ij_ieRsAt7Xt"
   },
   "outputs": [],
   "source": [
    "class Initializer:\n",
    "\n",
    "  def init_weights(self, n_in, n_out):\n",
    "    raise NotImplementedError\n",
    "\n",
    "  def init_bias(self, n_out):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eb18N5phuIha"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class NormalInitializer(Initializer):\n",
    "\n",
    "  def __init__(self, mean=0, std=0.1):\n",
    "    self.mean = mean\n",
    "    self.std = std\n",
    "\n",
    "  def init_weights(self, n_in, n_out):\n",
    "    return [[Var(random.gauss(self.mean, self.std)) for _ in range(n_out)] for _ in range(n_in)]\n",
    "\n",
    "  def init_bias(self, n_out):\n",
    "    return [Var(0.0) for _ in range(n_out)]\n",
    "\n",
    "class ConstantInitializer(Initializer):\n",
    "\n",
    "  def __init__(self, weight=1.0, bias=0.0):\n",
    "    self.weight = weight\n",
    "    self.bias = bias\n",
    "\n",
    "  def init_weights(self, n_in, n_out):\n",
    "    return [[Var(self.weight) for _ in range(n_out)] for _ in range(n_in)]\n",
    "\n",
    "  def init_bias(self, n_out):\n",
    "    return [Var(self.bias) for _ in range(n_out)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "jOLYGnZKuM6W"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, n_in: int, n_out: int, act_fn, initializer = NormalInitializer()):\n",
    "        self.weights = initializer.init_weights(n_in, n_out)\n",
    "        self.bias = initializer.init_bias(n_out)\n",
    "        self.act_fn = act_fn\n",
    "        \n",
    "    def __repr__(self):    \n",
    "        return 'Weights: ' + repr(self.weights) + ' Biases: ' + repr(self.bias)\n",
    "\n",
    "    def parameters(self) -> Sequence[Var]:\n",
    "      params = []\n",
    "      for r in self.weights:\n",
    "        params += r\n",
    "\n",
    "      return params + self.bias\n",
    "\n",
    "    def forward(self, single_input: Sequence[Var]) -> Sequence[Var]:\n",
    "        # self.weights is a matrix with dimension n_in x n_out. We check that the dimensionality of the input \n",
    "        # to the current layer matches the number of nodes in the current layer\n",
    "        assert len(self.weights) == len(single_input), \"weights and single_input must match in first dimension\"\n",
    "        weights = self.weights\n",
    "        out = []\n",
    "        # For some given data point single_input, we now want to calculate the resulting value in each node in the current layer\n",
    "        # We therefore loop over the (number of) nodes in the current layer:\n",
    "        for j in range(len(weights[0])): \n",
    "            # Initialize the node value depending on its corresponding parameters.\n",
    "            node = Var(0.0) # <- Insert code\n",
    "            # We now finish the linear transformation corresponding to the parameters of the currently considered node.\n",
    "            for i in range(len(single_input)):\n",
    "                node += Var(0.0)  # <- Insert code\n",
    "            node = self.act_fn(node)\n",
    "            out.append(node)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpIZPBpNI0pO"
   },
   "source": [
    "## Exercise f) Add more activation functions\n",
    "\n",
    "To have a full definition of the neural network, we must define an activation function for every layer. Several activation functions have been proposed and have different characteristics. In the Var class we have already defined the rectified linear init (relu). \n",
    " \n",
    "Implement the following activation functions in the Var class:\n",
    "\n",
    "* Identity: $$\\mathrm{identity}(x) = x$$\n",
    "* Hyperbolic tangent: $$\\tanh(x)$$\n",
    "* Sigmoid (or logistic function): $$\\mathrm{sigmoid}(x) = \\frac{1}{1.0 + \\exp(-x ) }$$  Hint: $\\mathrm{sigmoid}'(x)= \\mathrm{sigmoid}(x)(1-\\mathrm{sigmoid}(x))$.  \n",
    "\n",
    "Hint: You can seek inspiration in the relu method in the Var class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_8n_SKnIW2F"
   },
   "source": [
    "## Exercise g) Complete the forward pass\n",
    "\n",
    "In the code below we initialize a 1-5-1 network and pass the training set through it. *The forward method in DenseLayer is **not** complete*. It just outputs zeros right now. The method forward should perform an [affine transformation](https://en.wikipedia.org/wiki/Affine_transformation) on the input followed by an application of the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "xDEjtePxE7Mv",
    "outputId": "753406cd-d8a1-4282-ce03-25ad959b0e11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000)]]\n"
     ]
    }
   ],
   "source": [
    "NN = [\n",
    "    DenseLayer(1, 5, lambda x: x.relu()),\n",
    "    DenseLayer(5, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "def forward(input, network):\n",
    "\n",
    "  def forward_single(x, network):\n",
    "    for layer in network:\n",
    "        x = layer.forward(x)\n",
    "    return x\n",
    "\n",
    "  output = [ forward_single(input[n], network) for n in range(len(input))]\n",
    "  return output\n",
    "\n",
    "print(forward(x_train, NN))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLrGJytZFtGm"
   },
   "source": [
    "## Exercise h) Print all network parameters\n",
    "\n",
    "Make a function that prints all the parameters of the network (weights and biases) with information about in which layer they appear. In the object oriented spirit you should introduce a method in the DenseLayer class to print the parameters of a layer. Hint: You can take inspiration from the corresponding method in Var. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "iac-VwYGFtGm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 \n",
      " Weights: [[Var(v=0.1922, grad=0.0000), Var(v=0.0772, grad=0.0000), Var(v=0.0620, grad=0.0000), Var(v=0.0439, grad=0.0000), Var(v=-0.1413, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
      "Layer 1 \n",
      " Weights: [[Var(v=-0.2095, grad=0.0000)], [Var(v=-0.0826, grad=0.0000)], [Var(v=0.0915, grad=0.0000)], [Var(v=0.0304, grad=0.0000)], [Var(v=0.0788, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert code here and in the DenseLayer class\n",
    "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_79HOAXrFtHK"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "Now that we have defined our activation functions we can visualize them to see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1FcylHqLTl-Z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16f986e2f70>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-6, 6, 100)\n",
    "\n",
    "# convert from Var to ndarray  \n",
    "def Var_to_nparray(x):\n",
    "  y = np.zeros((len(x),len(x[0])))\n",
    "  for i in range(len(x)):\n",
    "    for j in range(len(x[0])):\n",
    "      y[i,j] = x[i][j].v\n",
    "  return y\n",
    "\n",
    "# define 1-1 network with weight = 1 and relu activation \n",
    "NN = [ DenseLayer(1, 1, lambda x: x.relu(), initializer = ConstantInitializer(1.0)) ] \n",
    "y = Var_to_nparray(forward(nparray_to_Var(x), NN))\n",
    "\n",
    "#y = Var_to_nparray(relu(nparray_to_Var(x)))\n",
    "plt.plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oOL2UolJFtHL"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Testing all activation layers\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "units = {\n",
    "    \"identity\": lambda x: x.identity(),\n",
    "    #\"sigmoid\": lambda x: x.sigmoid(),  <- uncomment before sharing\n",
    "    \"relu\": lambda x: x.relu(),\n",
    "    #\"tanh\": lambda x: x.tanh() <- uncomment before sharing\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "[plt.plot(x, Var_to_nparray(forward(nparray_to_Var(x), [DenseLayer(1, 1, unit, initializer = ConstantInitializer(1.0))]) ), label=unit_name, lw=2) for unit_name, unit in units.items()] # unit(nparray_to_Var(x))), label=unit_name, lw=2) for unit_name, unit in units.items()]\n",
    "plt.legend(loc=2, fontsize=16)\n",
    "plt.title('Our activation functions', fontsize=20)\n",
    "plt.ylim([-2, 5])\n",
    "plt.xlim([-6, 6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-jdEl-7FtGs"
   },
   "source": [
    "# Advanced initialization schemes\n",
    "\n",
    "If we are not careful with initialization, the signals we propagate forward ($a^{(l)}$, $l=1,\\ldots,L$) and backward ($\\delta^l$, $l=L,L-1,\\ldots,1$) can blow up or shrink to zero. A statistical analysis of the variance of the signals for different activation functions can be found in these two papers: [Glorot initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) and [He initialization](https://arxiv.org/pdf/1502.01852v1.pdf). \n",
    "\n",
    "The result of the analyses are proposals for how to make the initialization such that the variance of the signals (forward and backward) are kept approxmimatly constant when propagating from layer to layer. The exact expressions depend upon the non-linear activation function used. In Glorot initialization, the aim is to keep both the forward and backward variances constant whereas He only aims at keeping the variance in the forward pass constant.\n",
    "\n",
    "We define $n_{in}$ and $n_{out}$ as the number of input units and output units of a particular layer. \n",
    "\n",
    "The Glorot initialization has the form: \n",
    "\n",
    "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{2 \\alpha }{n_{in} + n_{out}} \\bigg) \\ . $$\n",
    "\n",
    "where $N(\\mu,\\sigma^2)$ is a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$ and $\\alpha$ is a parameter that depends upon the activation function used. For $\\tanh$, $\\alpha=1$ and for Rectified Linear Unit (ReLU) activations, $\\alpha=2$. (It is also possible to use a uniform distribution for initialization, see [this blog post](https://mmuratarat.github.io/2019-02-25/xavier-glorot-he-weight-init).) \n",
    "\n",
    "The He initialization is very similar\n",
    "\n",
    "$$w_{ij} \\sim N \\bigg( 0, \\, \\frac{\\alpha}{n_{in}} \\bigg) \\ . $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqeyab9qFtGs"
   },
   "source": [
    "## Exercise i) Glorot and He initialization\n",
    " \n",
    "Using the Initializer class, implement functions that implement Glorot and He \n",
    "\n",
    "Explain briefly how you would test numerically that these initializations have the sought after property. Hint: See plots in Glorot paper.\n",
    "\n",
    "Comment: If you want to be more advanced then try to make a universal initializer taking both the activation function and type (Glorot or He) as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Qyk01CgaFtGt"
   },
   "outputs": [],
   "source": [
    "## Glorot\n",
    "def DenseLayer_Glorot_tanh(n_in: int, n_out: int):\n",
    "  std = sqrt(2/n_in+n_out) # <- replace with proper initialization\n",
    "  return DenseLayer(n_in, n_out, lambda x: x.tanh(), initializer = NormalInitializer(std))\n",
    "\n",
    "## He\n",
    "def DenseLayer_He_relu(n_in: int, n_out: int):\n",
    "  std = sqrt(2/n_in) # <- replace with proper initialization\n",
    "  return DenseLayer(n_in, n_out, lambda x: x.relu(), initializer = NormalInitializer(std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XyXBD37FtHk"
   },
   "source": [
    "## Exercise j) Forward pass unit test\n",
    "\n",
    "Write a bit of code to make a unit test that the forward pass works. This can be done by defining a simple network with for example all weights equal to one (using the ConstantInitializer method) and identity activation functions. \n",
    "\n",
    "Hints: Use the [assert](https://www.w3schools.com/python/ref_keyword_assert.asp), the nparray_to_Var and the Var_to_nparray commands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0miqRUAFtHl"
   },
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faCxhfFnFtHp"
   },
   "source": [
    "# Loss functions\n",
    "\n",
    "We are only missing a loss function to we need to define a loss function and its derivative with respect to the output of the neural network $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "I2eDYKvAFtHq"
   },
   "outputs": [],
   "source": [
    "def squared_loss(t, y):\n",
    "  \n",
    "  # add check that sizes agree\n",
    "  \n",
    "  def squared_loss_single(t, y):\n",
    "    Loss = Var(0.0)\n",
    "    for i in range(len(t)): # sum over outputs\n",
    "      Loss += (t[i]-y[i]) ** 2\n",
    "    return Loss\n",
    "\n",
    "  Loss = Var(0.0)\n",
    "  for n in range(len(t)): # sum over training data\n",
    "    Loss += squared_loss_single(t[n],y[n])\n",
    "  return Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrwSJ2UWFtHu"
   },
   "source": [
    "## Exercise j) Implement cross entropy loss\n",
    "\n",
    "Insert code below to implement cross-entropy loss for general dimensionality of $t$. Use a logits formulation:\n",
    "$$\n",
    "\\rm{Loss} = - \\sum_i t_i \\, log \\, p_i \n",
    "$$\n",
    "with $p$ given by the the softmax function in terms of the logits $h$:\n",
    "$$\n",
    "p_i = \\frac{\\exp(h_i)}{\\sum_{i'} \\exp(h_{i'})} .\n",
    "$$\n",
    "Inserting $p$ in the expression for the loss gives\n",
    "$$\n",
    "\\rm{Loss} = - \\sum_i t_i h_i + \\rm{LogSumExp}(h) \\ ,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\rm{LogSumExp}(h) = \\log \\sum_i \\exp h_i \\ .\n",
    "$$\n",
    "This is true for $t$ being a one-hot vector. \n",
    "\n",
    "Call the function to convince yourself it works. \n",
    "\n",
    "In practice you want to implement a [numerically stable](https://leimao.github.io/blog/LogSumExp/) version of LogSumExp. But we will not bother about that here.\n",
    "\n",
    "Help: You can add these methods in the Var class:\n",
    "\n",
    "    def exp(self):\n",
    "        return Var(exp(self.v), lambda: [(self, exp(self.v))])\n",
    "    \n",
    "    def log(self):\n",
    "        return Var(log(self.v), lambda: [(self, self.v ** -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "6nMuxyfzFtHv"
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(t, h):\n",
    "     \n",
    "    Loss = Var(0.0)\n",
    "    # Insert code here\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fAF5ew4FtHy"
   },
   "source": [
    "# Backward pass\n",
    "\n",
    "Now the magic happens! We get the calculation of the gradients for free. Just do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "iHyfPPI9Qqwu"
   },
   "outputs": [],
   "source": [
    "NN = [\n",
    "    DenseLayer(1, 5, lambda x: x.relu()),\n",
    "    DenseLayer(5, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "output = forward(x_train, NN)\n",
    "\n",
    "Loss = squared_loss(y_train,output)\n",
    "Loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49biIAYKQ1oG"
   },
   "source": [
    "and the gradients will be calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "_rGt1bq_Q7uk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 \n",
      " Weights: [[Var(v=0.0947, grad=0.0000), Var(v=-0.2507, grad=0.0000), Var(v=-0.1119, grad=0.0000), Var(v=0.0179, grad=0.0000), Var(v=-0.0747, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
      "Layer 1 \n",
      " Weights: [[Var(v=-0.1466, grad=0.0000)], [Var(v=-0.0367, grad=0.0000)], [Var(v=0.2168, grad=0.0000)], [Var(v=-0.0893, grad=0.0000)], [Var(v=-0.0322, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7d7qK0uFtH9"
   },
   "source": [
    "# Backward pass unit test\n",
    "\n",
    "Above we used finite differences to test that Nanograd is actually doing what it is supposed to do. We can in principle try the same for the neural network. But we will trust that the test above is enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgBi8GOSFtIN"
   },
   "source": [
    "# Training and validation\n",
    "\n",
    "We are ready to train some neural networks!\n",
    "\n",
    "We initialize again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "01ePmzBzRtdh"
   },
   "outputs": [],
   "source": [
    "NN = [\n",
    "    DenseLayer(1, 15, lambda x: x.relu()),\n",
    "    DenseLayer(15, 50, lambda x: x.relu()),\n",
    "    DenseLayer(50, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "output = forward(x_train, NN)\n",
    "\n",
    "Loss = squared_loss(y_train,output)\n",
    "Loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10iRPiQ1ISHw"
   },
   "source": [
    "and make an update:\n",
    "\n",
    "We introduce a help function parameters to have a handle in all parameters in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "dhAI7eyeznia"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network before update:\n",
      "Layer 0 \n",
      " Weights: [[Var(v=0.0262, grad=0.0000), Var(v=0.0467, grad=0.0000), Var(v=-0.0064, grad=0.0000), Var(v=-0.0785, grad=0.0000), Var(v=-0.0141, grad=0.0000), Var(v=-0.1341, grad=0.0000), Var(v=0.0714, grad=0.0000), Var(v=-0.1879, grad=0.0000), Var(v=-0.0761, grad=0.0000), Var(v=0.0757, grad=0.0000), Var(v=-0.1493, grad=0.0000), Var(v=0.0084, grad=0.0000), Var(v=0.2500, grad=0.0000), Var(v=0.0600, grad=0.0000), Var(v=0.0567, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
      "Layer 1 \n",
      " Weights: [[Var(v=0.0958, grad=0.0000), Var(v=-0.1757, grad=0.0000), Var(v=-0.0777, grad=0.0000), Var(v=-0.1264, grad=0.0000), Var(v=0.0697, grad=0.0000), Var(v=0.0561, grad=0.0000), Var(v=-0.0201, grad=0.0000), Var(v=-0.0097, grad=0.0000), Var(v=0.0988, grad=0.0000), Var(v=0.0684, grad=0.0000), Var(v=0.0712, grad=0.0000), Var(v=-0.2111, grad=0.0000), Var(v=-0.0489, grad=0.0000), Var(v=-0.0360, grad=0.0000), Var(v=0.0627, grad=0.0000), Var(v=0.1811, grad=0.0000), Var(v=0.0055, grad=0.0000), Var(v=-0.0945, grad=0.0000), Var(v=-0.0113, grad=0.0000), Var(v=-0.1494, grad=0.0000), Var(v=-0.1049, grad=0.0000), Var(v=-0.2283, grad=0.0000), Var(v=0.1096, grad=0.0000), Var(v=-0.2114, grad=0.0000), Var(v=-0.0837, grad=0.0000), Var(v=0.0383, grad=0.0000), Var(v=0.0241, grad=0.0000), Var(v=-0.1216, grad=0.0000), Var(v=-0.0620, grad=0.0000), Var(v=0.1546, grad=0.0000), Var(v=-0.1381, grad=0.0000), Var(v=0.1335, grad=0.0000), Var(v=-0.0599, grad=0.0000), Var(v=0.0288, grad=0.0000), Var(v=0.0534, grad=0.0000), Var(v=-0.0990, grad=0.0000), Var(v=0.0911, grad=0.0000), Var(v=-0.1124, grad=0.0000), Var(v=0.0302, grad=0.0000), Var(v=0.0048, grad=0.0000), Var(v=0.0371, grad=0.0000), Var(v=0.0046, grad=0.0000), Var(v=0.0372, grad=0.0000), Var(v=0.0375, grad=0.0000), Var(v=0.0019, grad=0.0000), Var(v=0.1216, grad=0.0000), Var(v=-0.0320, grad=0.0000), Var(v=-0.1257, grad=0.0000), Var(v=-0.1636, grad=0.0000), Var(v=0.0333, grad=0.0000)], [Var(v=0.0928, grad=0.0000), Var(v=0.0450, grad=0.0000), Var(v=-0.2179, grad=0.0000), Var(v=0.0389, grad=0.0000), Var(v=0.1113, grad=0.0000), Var(v=-0.0066, grad=0.0000), Var(v=0.0630, grad=0.0000), Var(v=0.0258, grad=0.0000), Var(v=-0.0449, grad=0.0000), Var(v=-0.0697, grad=0.0000), Var(v=-0.0685, grad=0.0000), Var(v=0.0208, grad=0.0000), Var(v=0.1085, grad=0.0000), Var(v=0.0297, grad=0.0000), Var(v=0.1481, grad=0.0000), Var(v=-0.0053, grad=0.0000), Var(v=0.1157, grad=0.0000), Var(v=0.0079, grad=0.0000), Var(v=-0.0646, grad=0.0000), Var(v=0.0020, grad=0.0000), Var(v=0.0469, grad=0.0000), Var(v=-0.1633, grad=0.0000), Var(v=0.1790, grad=0.0000), Var(v=0.0133, grad=0.0000), Var(v=-0.1474, grad=0.0000), Var(v=-0.0265, grad=0.0000), Var(v=-0.1486, grad=0.0000), Var(v=0.0127, grad=0.0000), Var(v=0.1152, grad=0.0000), Var(v=0.0173, grad=0.0000), Var(v=-0.0433, grad=0.0000), Var(v=-0.0603, grad=0.0000), Var(v=0.0793, grad=0.0000), Var(v=0.1242, grad=0.0000), Var(v=0.0079, grad=0.0000), Var(v=-0.1331, grad=0.0000), Var(v=-0.0176, grad=0.0000), Var(v=0.2073, grad=0.0000), Var(v=-0.1025, grad=0.0000), Var(v=-0.0077, grad=0.0000), Var(v=0.0662, grad=0.0000), Var(v=0.1535, grad=0.0000), Var(v=0.0145, grad=0.0000), Var(v=0.0679, grad=0.0000), Var(v=-0.1390, grad=0.0000), Var(v=-0.0778, grad=0.0000), Var(v=0.0377, grad=0.0000), Var(v=0.0378, grad=0.0000), Var(v=0.0898, grad=0.0000), Var(v=-0.0604, grad=0.0000)], [Var(v=-0.0691, grad=0.0000), Var(v=-0.2256, grad=0.0000), Var(v=0.0354, grad=0.0000), Var(v=-0.0757, grad=0.0000), Var(v=-0.1173, grad=0.0000), Var(v=0.0311, grad=0.0000), Var(v=-0.0817, grad=0.0000), Var(v=-0.0061, grad=0.0000), Var(v=0.0004, grad=0.0000), Var(v=0.0808, grad=0.0000), Var(v=-0.0961, grad=0.0000), Var(v=0.0217, grad=0.0000), Var(v=0.1425, grad=0.0000), Var(v=-0.1454, grad=0.0000), Var(v=-0.1183, grad=0.0000), Var(v=0.0390, grad=0.0000), Var(v=0.1028, grad=0.0000), Var(v=0.0007, grad=0.0000), Var(v=-0.1319, grad=0.0000), Var(v=0.1592, grad=0.0000), Var(v=0.0141, grad=0.0000), Var(v=0.0317, grad=0.0000), Var(v=-0.0738, grad=0.0000), Var(v=0.0268, grad=0.0000), Var(v=0.1002, grad=0.0000), Var(v=-0.0454, grad=0.0000), Var(v=-0.0553, grad=0.0000), Var(v=-0.1261, grad=0.0000), Var(v=0.0262, grad=0.0000), Var(v=-0.1149, grad=0.0000), Var(v=0.0042, grad=0.0000), Var(v=-0.0455, grad=0.0000), Var(v=-0.2155, grad=0.0000), Var(v=0.1326, grad=0.0000), Var(v=0.0098, grad=0.0000), Var(v=0.0175, grad=0.0000), Var(v=-0.1357, grad=0.0000), Var(v=0.0077, grad=0.0000), Var(v=0.0838, grad=0.0000), Var(v=-0.1161, grad=0.0000), Var(v=-0.0175, grad=0.0000), Var(v=-0.1210, grad=0.0000), Var(v=0.0705, grad=0.0000), Var(v=0.0875, grad=0.0000), Var(v=0.1083, grad=0.0000), Var(v=-0.0585, grad=0.0000), Var(v=0.1497, grad=0.0000), Var(v=-0.0380, grad=0.0000), Var(v=0.1667, grad=0.0000), Var(v=0.0439, grad=0.0000)], [Var(v=-0.0705, grad=0.0000), Var(v=0.1076, grad=0.0000), Var(v=0.0090, grad=0.0000), Var(v=0.0933, grad=0.0000), Var(v=0.0264, grad=0.0000), Var(v=0.1047, grad=0.0000), Var(v=0.0852, grad=0.0000), Var(v=0.0351, grad=0.0000), Var(v=-0.0016, grad=0.0000), Var(v=0.1529, grad=0.0000), Var(v=0.0525, grad=0.0000), Var(v=-0.0975, grad=0.0000), Var(v=-0.0415, grad=0.0000), Var(v=-0.0578, grad=0.0000), Var(v=-0.0433, grad=0.0000), Var(v=-0.0020, grad=0.0000), Var(v=0.1206, grad=0.0000), Var(v=-0.0003, grad=0.0000), Var(v=-0.0446, grad=0.0000), Var(v=-0.2148, grad=0.0000), Var(v=0.0267, grad=0.0000), Var(v=0.1061, grad=0.0000), Var(v=0.0266, grad=0.0000), Var(v=0.0786, grad=0.0000), Var(v=-0.1724, grad=0.0000), Var(v=-0.1608, grad=0.0000), Var(v=0.0494, grad=0.0000), Var(v=-0.0479, grad=0.0000), Var(v=-0.0381, grad=0.0000), Var(v=-0.0264, grad=0.0000), Var(v=0.0292, grad=0.0000), Var(v=-0.2154, grad=0.0000), Var(v=0.0132, grad=0.0000), Var(v=0.1219, grad=0.0000), Var(v=0.0226, grad=0.0000), Var(v=-0.0729, grad=0.0000), Var(v=0.0703, grad=0.0000), Var(v=-0.0225, grad=0.0000), Var(v=-0.0218, grad=0.0000), Var(v=-0.0774, grad=0.0000), Var(v=0.1195, grad=0.0000), Var(v=0.1038, grad=0.0000), Var(v=-0.0412, grad=0.0000), Var(v=-0.0082, grad=0.0000), Var(v=0.0417, grad=0.0000), Var(v=-0.0709, grad=0.0000), Var(v=0.0060, grad=0.0000), Var(v=-0.0553, grad=0.0000), Var(v=0.0200, grad=0.0000), Var(v=0.0211, grad=0.0000)], [Var(v=-0.0520, grad=0.0000), Var(v=-0.1404, grad=0.0000), Var(v=0.0480, grad=0.0000), Var(v=-0.1845, grad=0.0000), Var(v=0.0589, grad=0.0000), Var(v=-0.0530, grad=0.0000), Var(v=-0.0041, grad=0.0000), Var(v=0.0083, grad=0.0000), Var(v=-0.0150, grad=0.0000), Var(v=-0.0491, grad=0.0000), Var(v=0.1347, grad=0.0000), Var(v=-0.0475, grad=0.0000), Var(v=0.0744, grad=0.0000), Var(v=0.1985, grad=0.0000), Var(v=0.1736, grad=0.0000), Var(v=0.0555, grad=0.0000), Var(v=0.0756, grad=0.0000), Var(v=-0.1431, grad=0.0000), Var(v=0.0572, grad=0.0000), Var(v=-0.0068, grad=0.0000), Var(v=0.0848, grad=0.0000), Var(v=0.0437, grad=0.0000), Var(v=0.0194, grad=0.0000), Var(v=0.0525, grad=0.0000), Var(v=-0.0295, grad=0.0000), Var(v=-0.1381, grad=0.0000), Var(v=-0.0098, grad=0.0000), Var(v=-0.0299, grad=0.0000), Var(v=-0.1061, grad=0.0000), Var(v=-0.0988, grad=0.0000), Var(v=-0.0597, grad=0.0000), Var(v=0.1287, grad=0.0000), Var(v=0.1162, grad=0.0000), Var(v=0.1105, grad=0.0000), Var(v=0.1783, grad=0.0000), Var(v=0.0267, grad=0.0000), Var(v=0.0459, grad=0.0000), Var(v=-0.0244, grad=0.0000), Var(v=0.0263, grad=0.0000), Var(v=-0.1805, grad=0.0000), Var(v=-0.0762, grad=0.0000), Var(v=0.1147, grad=0.0000), Var(v=-0.0283, grad=0.0000), Var(v=-0.2016, grad=0.0000), Var(v=0.0632, grad=0.0000), Var(v=0.1268, grad=0.0000), Var(v=-0.0642, grad=0.0000), Var(v=0.0718, grad=0.0000), Var(v=0.0182, grad=0.0000), Var(v=0.0158, grad=0.0000)], [Var(v=-0.0980, grad=0.0000), Var(v=0.2393, grad=0.0000), Var(v=-0.0376, grad=0.0000), Var(v=0.0504, grad=0.0000), Var(v=-0.2026, grad=0.0000), Var(v=0.1441, grad=0.0000), Var(v=-0.0539, grad=0.0000), Var(v=0.1183, grad=0.0000), Var(v=0.0041, grad=0.0000), Var(v=-0.0283, grad=0.0000), Var(v=-0.0683, grad=0.0000), Var(v=0.0227, grad=0.0000), Var(v=-0.0601, grad=0.0000), Var(v=0.0853, grad=0.0000), Var(v=-0.0053, grad=0.0000), Var(v=-0.0306, grad=0.0000), Var(v=-0.1458, grad=0.0000), Var(v=-0.0360, grad=0.0000), Var(v=-0.0853, grad=0.0000), Var(v=-0.0603, grad=0.0000), Var(v=0.0623, grad=0.0000), Var(v=-0.0806, grad=0.0000), Var(v=-0.1422, grad=0.0000), Var(v=-0.1307, grad=0.0000), Var(v=-0.1552, grad=0.0000), Var(v=-0.0904, grad=0.0000), Var(v=-0.0597, grad=0.0000), Var(v=-0.1896, grad=0.0000), Var(v=0.0715, grad=0.0000), Var(v=0.1101, grad=0.0000), Var(v=0.0361, grad=0.0000), Var(v=-0.1109, grad=0.0000), Var(v=-0.0306, grad=0.0000), Var(v=0.0846, grad=0.0000), Var(v=0.0117, grad=0.0000), Var(v=-0.1056, grad=0.0000), Var(v=-0.0437, grad=0.0000), Var(v=-0.0173, grad=0.0000), Var(v=0.0642, grad=0.0000), Var(v=-0.0385, grad=0.0000), Var(v=0.1043, grad=0.0000), Var(v=-0.1284, grad=0.0000), Var(v=0.1843, grad=0.0000), Var(v=0.1769, grad=0.0000), Var(v=0.1634, grad=0.0000), Var(v=0.1078, grad=0.0000), Var(v=0.1239, grad=0.0000), Var(v=-0.1447, grad=0.0000), Var(v=-0.0350, grad=0.0000), Var(v=-0.0107, grad=0.0000)], [Var(v=-0.0795, grad=0.0000), Var(v=-0.2219, grad=0.0000), Var(v=0.1430, grad=0.0000), Var(v=-0.0663, grad=0.0000), Var(v=0.2075, grad=0.0000), Var(v=-0.0064, grad=0.0000), Var(v=-0.0415, grad=0.0000), Var(v=0.0205, grad=0.0000), Var(v=-0.0158, grad=0.0000), Var(v=-0.0910, grad=0.0000), Var(v=-0.1394, grad=0.0000), Var(v=0.1515, grad=0.0000), Var(v=-0.1110, grad=0.0000), Var(v=0.1206, grad=0.0000), Var(v=-0.0600, grad=0.0000), Var(v=0.0870, grad=0.0000), Var(v=-0.1673, grad=0.0000), Var(v=-0.1159, grad=0.0000), Var(v=0.1671, grad=0.0000), Var(v=-0.0764, grad=0.0000), Var(v=0.1763, grad=0.0000), Var(v=0.0856, grad=0.0000), Var(v=0.0587, grad=0.0000), Var(v=0.0572, grad=0.0000), Var(v=0.1165, grad=0.0000), Var(v=0.0509, grad=0.0000), Var(v=0.0474, grad=0.0000), Var(v=-0.0012, grad=0.0000), Var(v=0.0915, grad=0.0000), Var(v=-0.0015, grad=0.0000), Var(v=-0.1006, grad=0.0000), Var(v=-0.1111, grad=0.0000), Var(v=-0.1479, grad=0.0000), Var(v=-0.1197, grad=0.0000), Var(v=0.0436, grad=0.0000), Var(v=-0.0062, grad=0.0000), Var(v=0.0389, grad=0.0000), Var(v=-0.1168, grad=0.0000), Var(v=0.1300, grad=0.0000), Var(v=0.0299, grad=0.0000), Var(v=0.0913, grad=0.0000), Var(v=0.1020, grad=0.0000), Var(v=0.0024, grad=0.0000), Var(v=-0.1494, grad=0.0000), Var(v=0.0071, grad=0.0000), Var(v=-0.0044, grad=0.0000), Var(v=0.0427, grad=0.0000), Var(v=0.0523, grad=0.0000), Var(v=-0.0360, grad=0.0000), Var(v=0.0925, grad=0.0000)], [Var(v=0.0360, grad=0.0000), Var(v=-0.0872, grad=0.0000), Var(v=0.0844, grad=0.0000), Var(v=-0.0483, grad=0.0000), Var(v=0.2329, grad=0.0000), Var(v=0.1712, grad=0.0000), Var(v=0.0573, grad=0.0000), Var(v=-0.0471, grad=0.0000), Var(v=0.1333, grad=0.0000), Var(v=-0.2557, grad=0.0000), Var(v=-0.0678, grad=0.0000), Var(v=0.1786, grad=0.0000), Var(v=-0.0824, grad=0.0000), Var(v=-0.0995, grad=0.0000), Var(v=-0.0103, grad=0.0000), Var(v=-0.0275, grad=0.0000), Var(v=-0.0375, grad=0.0000), Var(v=0.1337, grad=0.0000), Var(v=-0.0535, grad=0.0000), Var(v=0.0802, grad=0.0000), Var(v=-0.1159, grad=0.0000), Var(v=-0.0804, grad=0.0000), Var(v=-0.0580, grad=0.0000), Var(v=-0.1448, grad=0.0000), Var(v=0.0110, grad=0.0000), Var(v=0.1639, grad=0.0000), Var(v=-0.0498, grad=0.0000), Var(v=0.1139, grad=0.0000), Var(v=-0.0194, grad=0.0000), Var(v=-0.0358, grad=0.0000), Var(v=0.0035, grad=0.0000), Var(v=0.0584, grad=0.0000), Var(v=0.0052, grad=0.0000), Var(v=-0.2370, grad=0.0000), Var(v=0.2296, grad=0.0000), Var(v=0.0426, grad=0.0000), Var(v=-0.1402, grad=0.0000), Var(v=-0.1375, grad=0.0000), Var(v=0.0823, grad=0.0000), Var(v=-0.0594, grad=0.0000), Var(v=-0.1271, grad=0.0000), Var(v=-0.0090, grad=0.0000), Var(v=0.0291, grad=0.0000), Var(v=0.0410, grad=0.0000), Var(v=-0.0237, grad=0.0000), Var(v=-0.1335, grad=0.0000), Var(v=0.1442, grad=0.0000), Var(v=0.0152, grad=0.0000), Var(v=-0.0495, grad=0.0000), Var(v=0.0113, grad=0.0000)], [Var(v=-0.0230, grad=0.0000), Var(v=-0.0974, grad=0.0000), Var(v=0.0363, grad=0.0000), Var(v=-0.0911, grad=0.0000), Var(v=-0.0147, grad=0.0000), Var(v=0.0216, grad=0.0000), Var(v=-0.0704, grad=0.0000), Var(v=-0.1212, grad=0.0000), Var(v=-0.0641, grad=0.0000), Var(v=-0.1011, grad=0.0000), Var(v=-0.2170, grad=0.0000), Var(v=-0.0477, grad=0.0000), Var(v=-0.0769, grad=0.0000), Var(v=-0.1381, grad=0.0000), Var(v=0.0459, grad=0.0000), Var(v=-0.1407, grad=0.0000), Var(v=0.0625, grad=0.0000), Var(v=0.0976, grad=0.0000), Var(v=0.1410, grad=0.0000), Var(v=0.0494, grad=0.0000), Var(v=-0.0647, grad=0.0000), Var(v=0.0599, grad=0.0000), Var(v=-0.1083, grad=0.0000), Var(v=0.0549, grad=0.0000), Var(v=0.1428, grad=0.0000), Var(v=-0.0594, grad=0.0000), Var(v=-0.0057, grad=0.0000), Var(v=0.1400, grad=0.0000), Var(v=-0.0099, grad=0.0000), Var(v=0.0514, grad=0.0000), Var(v=-0.1530, grad=0.0000), Var(v=0.1448, grad=0.0000), Var(v=0.0045, grad=0.0000), Var(v=0.1521, grad=0.0000), Var(v=-0.0457, grad=0.0000), Var(v=0.0924, grad=0.0000), Var(v=0.0095, grad=0.0000), Var(v=-0.0012, grad=0.0000), Var(v=0.2218, grad=0.0000), Var(v=0.0399, grad=0.0000), Var(v=0.0732, grad=0.0000), Var(v=-0.1548, grad=0.0000), Var(v=-0.0953, grad=0.0000), Var(v=-0.1056, grad=0.0000), Var(v=-0.0484, grad=0.0000), Var(v=-0.1120, grad=0.0000), Var(v=-0.0173, grad=0.0000), Var(v=0.0895, grad=0.0000), Var(v=0.0286, grad=0.0000), Var(v=0.0044, grad=0.0000)], [Var(v=-0.0185, grad=0.0000), Var(v=0.0904, grad=0.0000), Var(v=0.1375, grad=0.0000), Var(v=-0.0327, grad=0.0000), Var(v=0.0297, grad=0.0000), Var(v=0.0270, grad=0.0000), Var(v=0.2811, grad=0.0000), Var(v=0.1674, grad=0.0000), Var(v=0.0065, grad=0.0000), Var(v=-0.0045, grad=0.0000), Var(v=-0.0988, grad=0.0000), Var(v=-0.3583, grad=0.0000), Var(v=0.0038, grad=0.0000), Var(v=0.0126, grad=0.0000), Var(v=-0.1645, grad=0.0000), Var(v=0.0446, grad=0.0000), Var(v=-0.0258, grad=0.0000), Var(v=-0.1593, grad=0.0000), Var(v=-0.1192, grad=0.0000), Var(v=-0.0009, grad=0.0000), Var(v=-0.0923, grad=0.0000), Var(v=0.2304, grad=0.0000), Var(v=0.1234, grad=0.0000), Var(v=0.0919, grad=0.0000), Var(v=0.0956, grad=0.0000), Var(v=-0.1140, grad=0.0000), Var(v=-0.1327, grad=0.0000), Var(v=-0.1356, grad=0.0000), Var(v=-0.0018, grad=0.0000), Var(v=0.1747, grad=0.0000), Var(v=0.1498, grad=0.0000), Var(v=-0.0247, grad=0.0000), Var(v=0.0976, grad=0.0000), Var(v=0.2138, grad=0.0000), Var(v=-0.0298, grad=0.0000), Var(v=0.0858, grad=0.0000), Var(v=-0.0518, grad=0.0000), Var(v=-0.0629, grad=0.0000), Var(v=-0.0960, grad=0.0000), Var(v=-0.0173, grad=0.0000), Var(v=0.0789, grad=0.0000), Var(v=-0.0126, grad=0.0000), Var(v=-0.0424, grad=0.0000), Var(v=-0.0332, grad=0.0000), Var(v=0.0017, grad=0.0000), Var(v=0.0388, grad=0.0000), Var(v=-0.1861, grad=0.0000), Var(v=0.0564, grad=0.0000), Var(v=-0.1230, grad=0.0000), Var(v=-0.0993, grad=0.0000)], [Var(v=-0.0680, grad=0.0000), Var(v=0.0613, grad=0.0000), Var(v=0.0589, grad=0.0000), Var(v=0.2238, grad=0.0000), Var(v=-0.0179, grad=0.0000), Var(v=-0.0599, grad=0.0000), Var(v=-0.0493, grad=0.0000), Var(v=0.0222, grad=0.0000), Var(v=-0.0795, grad=0.0000), Var(v=0.0636, grad=0.0000), Var(v=-0.0619, grad=0.0000), Var(v=-0.0031, grad=0.0000), Var(v=0.0873, grad=0.0000), Var(v=0.1240, grad=0.0000), Var(v=-0.0777, grad=0.0000), Var(v=0.0177, grad=0.0000), Var(v=0.0226, grad=0.0000), Var(v=-0.1396, grad=0.0000), Var(v=0.1780, grad=0.0000), Var(v=0.0495, grad=0.0000), Var(v=-0.0366, grad=0.0000), Var(v=0.0979, grad=0.0000), Var(v=0.0299, grad=0.0000), Var(v=0.0489, grad=0.0000), Var(v=-0.0806, grad=0.0000), Var(v=-0.1319, grad=0.0000), Var(v=0.1019, grad=0.0000), Var(v=0.0998, grad=0.0000), Var(v=0.0368, grad=0.0000), Var(v=0.1133, grad=0.0000), Var(v=0.0127, grad=0.0000), Var(v=0.1236, grad=0.0000), Var(v=-0.0862, grad=0.0000), Var(v=-0.0992, grad=0.0000), Var(v=0.1581, grad=0.0000), Var(v=-0.0984, grad=0.0000), Var(v=0.0457, grad=0.0000), Var(v=0.2303, grad=0.0000), Var(v=-0.0273, grad=0.0000), Var(v=0.0196, grad=0.0000), Var(v=0.0721, grad=0.0000), Var(v=-0.0724, grad=0.0000), Var(v=0.0981, grad=0.0000), Var(v=0.0664, grad=0.0000), Var(v=0.0816, grad=0.0000), Var(v=-0.0720, grad=0.0000), Var(v=-0.0270, grad=0.0000), Var(v=0.0303, grad=0.0000), Var(v=0.0388, grad=0.0000), Var(v=-0.1155, grad=0.0000)], [Var(v=0.1001, grad=0.0000), Var(v=-0.0586, grad=0.0000), Var(v=-0.0263, grad=0.0000), Var(v=0.0554, grad=0.0000), Var(v=-0.0463, grad=0.0000), Var(v=0.1039, grad=0.0000), Var(v=-0.0805, grad=0.0000), Var(v=-0.0836, grad=0.0000), Var(v=0.0150, grad=0.0000), Var(v=0.0194, grad=0.0000), Var(v=-0.0841, grad=0.0000), Var(v=0.2141, grad=0.0000), Var(v=-0.0417, grad=0.0000), Var(v=0.1512, grad=0.0000), Var(v=0.1732, grad=0.0000), Var(v=0.1146, grad=0.0000), Var(v=0.0235, grad=0.0000), Var(v=-0.0735, grad=0.0000), Var(v=-0.0125, grad=0.0000), Var(v=-0.1590, grad=0.0000), Var(v=-0.0597, grad=0.0000), Var(v=-0.0559, grad=0.0000), Var(v=0.0912, grad=0.0000), Var(v=0.1122, grad=0.0000), Var(v=-0.0250, grad=0.0000), Var(v=-0.0161, grad=0.0000), Var(v=0.1177, grad=0.0000), Var(v=0.1163, grad=0.0000), Var(v=0.1896, grad=0.0000), Var(v=-0.0351, grad=0.0000), Var(v=0.0085, grad=0.0000), Var(v=0.1746, grad=0.0000), Var(v=-0.0271, grad=0.0000), Var(v=-0.0848, grad=0.0000), Var(v=-0.0202, grad=0.0000), Var(v=-0.0666, grad=0.0000), Var(v=0.0525, grad=0.0000), Var(v=-0.0695, grad=0.0000), Var(v=0.1094, grad=0.0000), Var(v=0.0287, grad=0.0000), Var(v=-0.0192, grad=0.0000), Var(v=-0.0388, grad=0.0000), Var(v=-0.0459, grad=0.0000), Var(v=0.0071, grad=0.0000), Var(v=0.0091, grad=0.0000), Var(v=0.0014, grad=0.0000), Var(v=0.0843, grad=0.0000), Var(v=-0.2273, grad=0.0000), Var(v=0.0313, grad=0.0000), Var(v=0.0226, grad=0.0000)], [Var(v=-0.1682, grad=0.0000), Var(v=-0.1121, grad=0.0000), Var(v=-0.0142, grad=0.0000), Var(v=-0.0005, grad=0.0000), Var(v=-0.1236, grad=0.0000), Var(v=-0.1059, grad=0.0000), Var(v=-0.0699, grad=0.0000), Var(v=-0.1441, grad=0.0000), Var(v=0.0391, grad=0.0000), Var(v=-0.1460, grad=0.0000), Var(v=0.0377, grad=0.0000), Var(v=0.0293, grad=0.0000), Var(v=0.1150, grad=0.0000), Var(v=0.0328, grad=0.0000), Var(v=-0.1097, grad=0.0000), Var(v=-0.0012, grad=0.0000), Var(v=-0.1222, grad=0.0000), Var(v=-0.0782, grad=0.0000), Var(v=0.0535, grad=0.0000), Var(v=-0.0098, grad=0.0000), Var(v=0.0153, grad=0.0000), Var(v=0.0509, grad=0.0000), Var(v=-0.1471, grad=0.0000), Var(v=-0.0561, grad=0.0000), Var(v=0.1645, grad=0.0000), Var(v=0.0443, grad=0.0000), Var(v=0.1114, grad=0.0000), Var(v=-0.1016, grad=0.0000), Var(v=-0.0728, grad=0.0000), Var(v=0.0800, grad=0.0000), Var(v=0.0817, grad=0.0000), Var(v=0.0600, grad=0.0000), Var(v=-0.0198, grad=0.0000), Var(v=0.0451, grad=0.0000), Var(v=-0.0342, grad=0.0000), Var(v=-0.1045, grad=0.0000), Var(v=-0.0027, grad=0.0000), Var(v=0.0809, grad=0.0000), Var(v=0.0238, grad=0.0000), Var(v=0.0925, grad=0.0000), Var(v=0.0710, grad=0.0000), Var(v=0.0058, grad=0.0000), Var(v=0.0723, grad=0.0000), Var(v=0.0275, grad=0.0000), Var(v=-0.0573, grad=0.0000), Var(v=0.1107, grad=0.0000), Var(v=-0.0611, grad=0.0000), Var(v=0.0160, grad=0.0000), Var(v=0.1622, grad=0.0000), Var(v=-0.0427, grad=0.0000)], [Var(v=0.0430, grad=0.0000), Var(v=-0.0656, grad=0.0000), Var(v=0.0600, grad=0.0000), Var(v=-0.1209, grad=0.0000), Var(v=0.0129, grad=0.0000), Var(v=-0.0592, grad=0.0000), Var(v=0.0400, grad=0.0000), Var(v=-0.0171, grad=0.0000), Var(v=-0.1256, grad=0.0000), Var(v=-0.0797, grad=0.0000), Var(v=0.0057, grad=0.0000), Var(v=0.0897, grad=0.0000), Var(v=-0.0859, grad=0.0000), Var(v=0.1830, grad=0.0000), Var(v=-0.1157, grad=0.0000), Var(v=0.1066, grad=0.0000), Var(v=-0.0889, grad=0.0000), Var(v=0.0805, grad=0.0000), Var(v=-0.0596, grad=0.0000), Var(v=0.0495, grad=0.0000), Var(v=-0.1089, grad=0.0000), Var(v=0.0261, grad=0.0000), Var(v=-0.1168, grad=0.0000), Var(v=0.0589, grad=0.0000), Var(v=-0.0218, grad=0.0000), Var(v=0.0482, grad=0.0000), Var(v=-0.1228, grad=0.0000), Var(v=-0.0760, grad=0.0000), Var(v=0.0733, grad=0.0000), Var(v=-0.0073, grad=0.0000), Var(v=0.0767, grad=0.0000), Var(v=-0.0488, grad=0.0000), Var(v=0.0216, grad=0.0000), Var(v=-0.1021, grad=0.0000), Var(v=-0.1707, grad=0.0000), Var(v=0.1442, grad=0.0000), Var(v=-0.1705, grad=0.0000), Var(v=-0.0926, grad=0.0000), Var(v=0.1652, grad=0.0000), Var(v=-0.1452, grad=0.0000), Var(v=0.0562, grad=0.0000), Var(v=0.0124, grad=0.0000), Var(v=-0.0935, grad=0.0000), Var(v=0.0673, grad=0.0000), Var(v=-0.0336, grad=0.0000), Var(v=-0.0298, grad=0.0000), Var(v=0.0311, grad=0.0000), Var(v=0.0384, grad=0.0000), Var(v=-0.0260, grad=0.0000), Var(v=-0.0669, grad=0.0000)], [Var(v=0.2684, grad=0.0000), Var(v=0.1023, grad=0.0000), Var(v=0.0080, grad=0.0000), Var(v=-0.1075, grad=0.0000), Var(v=0.1242, grad=0.0000), Var(v=-0.0436, grad=0.0000), Var(v=-0.0564, grad=0.0000), Var(v=-0.1048, grad=0.0000), Var(v=0.2006, grad=0.0000), Var(v=-0.1082, grad=0.0000), Var(v=-0.0278, grad=0.0000), Var(v=0.1813, grad=0.0000), Var(v=-0.0246, grad=0.0000), Var(v=0.0153, grad=0.0000), Var(v=0.2094, grad=0.0000), Var(v=0.0068, grad=0.0000), Var(v=0.0519, grad=0.0000), Var(v=0.0068, grad=0.0000), Var(v=0.1407, grad=0.0000), Var(v=-0.2884, grad=0.0000), Var(v=0.2353, grad=0.0000), Var(v=0.0601, grad=0.0000), Var(v=0.1291, grad=0.0000), Var(v=-0.0513, grad=0.0000), Var(v=-0.1170, grad=0.0000), Var(v=0.0141, grad=0.0000), Var(v=0.0304, grad=0.0000), Var(v=0.0139, grad=0.0000), Var(v=-0.0886, grad=0.0000), Var(v=0.0091, grad=0.0000), Var(v=0.0241, grad=0.0000), Var(v=-0.0375, grad=0.0000), Var(v=-0.0049, grad=0.0000), Var(v=-0.0360, grad=0.0000), Var(v=-0.0639, grad=0.0000), Var(v=0.2345, grad=0.0000), Var(v=0.0406, grad=0.0000), Var(v=-0.0911, grad=0.0000), Var(v=-0.0315, grad=0.0000), Var(v=0.0355, grad=0.0000), Var(v=0.0027, grad=0.0000), Var(v=0.0334, grad=0.0000), Var(v=-0.0117, grad=0.0000), Var(v=-0.1197, grad=0.0000), Var(v=0.1534, grad=0.0000), Var(v=-0.0844, grad=0.0000), Var(v=0.1814, grad=0.0000), Var(v=-0.0674, grad=0.0000), Var(v=-0.0882, grad=0.0000), Var(v=0.0766, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
      "Layer 2 \n",
      " Weights: [[Var(v=0.1259, grad=0.0000)], [Var(v=0.1001, grad=0.0000)], [Var(v=0.0469, grad=0.0000)], [Var(v=0.0235, grad=0.0000)], [Var(v=-0.1436, grad=0.0000)], [Var(v=-0.0939, grad=0.0000)], [Var(v=-0.0325, grad=0.0000)], [Var(v=0.0222, grad=0.0000)], [Var(v=-0.0984, grad=0.0000)], [Var(v=0.0655, grad=0.0000)], [Var(v=0.0555, grad=0.0000)], [Var(v=0.0674, grad=0.0000)], [Var(v=-0.0067, grad=0.0000)], [Var(v=-0.0809, grad=0.0000)], [Var(v=0.0027, grad=0.0000)], [Var(v=-0.1017, grad=0.0000)], [Var(v=0.1347, grad=0.0000)], [Var(v=0.1161, grad=0.0000)], [Var(v=-0.0783, grad=0.0000)], [Var(v=-0.0714, grad=0.0000)], [Var(v=0.0044, grad=0.0000)], [Var(v=0.0474, grad=0.0000)], [Var(v=0.0914, grad=0.0000)], [Var(v=-0.0201, grad=0.0000)], [Var(v=-0.0661, grad=0.0000)], [Var(v=0.0872, grad=0.0000)], [Var(v=-0.0453, grad=0.0000)], [Var(v=0.0057, grad=0.0000)], [Var(v=0.1406, grad=0.0000)], [Var(v=-0.0051, grad=0.0000)], [Var(v=-0.1653, grad=0.0000)], [Var(v=0.1622, grad=0.0000)], [Var(v=-0.0684, grad=0.0000)], [Var(v=0.0205, grad=0.0000)], [Var(v=0.0438, grad=0.0000)], [Var(v=-0.1548, grad=0.0000)], [Var(v=-0.1727, grad=0.0000)], [Var(v=0.0205, grad=0.0000)], [Var(v=-0.0046, grad=0.0000)], [Var(v=-0.0635, grad=0.0000)], [Var(v=0.0127, grad=0.0000)], [Var(v=-0.1018, grad=0.0000)], [Var(v=0.0273, grad=0.0000)], [Var(v=-0.0590, grad=0.0000)], [Var(v=-0.1019, grad=0.0000)], [Var(v=0.0644, grad=0.0000)], [Var(v=-0.0768, grad=0.0000)], [Var(v=-0.0693, grad=0.0000)], [Var(v=-0.2110, grad=0.0000)], [Var(v=-0.0403, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000)]\n",
      "\n",
      "Network after update:\n",
      "Layer 0 \n",
      " Weights: [[Var(v=0.0262, grad=0.0000), Var(v=0.0467, grad=0.0000), Var(v=-0.0064, grad=0.0000), Var(v=-0.0785, grad=0.0000), Var(v=-0.0141, grad=0.0000), Var(v=-0.1341, grad=0.0000), Var(v=0.0714, grad=0.0000), Var(v=-0.1879, grad=0.0000), Var(v=-0.0761, grad=0.0000), Var(v=0.0757, grad=0.0000), Var(v=-0.1493, grad=0.0000), Var(v=0.0084, grad=0.0000), Var(v=0.2500, grad=0.0000), Var(v=0.0600, grad=0.0000), Var(v=0.0567, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
      "Layer 1 \n",
      " Weights: [[Var(v=0.0958, grad=0.0000), Var(v=-0.1757, grad=0.0000), Var(v=-0.0777, grad=0.0000), Var(v=-0.1264, grad=0.0000), Var(v=0.0697, grad=0.0000), Var(v=0.0561, grad=0.0000), Var(v=-0.0201, grad=0.0000), Var(v=-0.0097, grad=0.0000), Var(v=0.0988, grad=0.0000), Var(v=0.0684, grad=0.0000), Var(v=0.0712, grad=0.0000), Var(v=-0.2111, grad=0.0000), Var(v=-0.0489, grad=0.0000), Var(v=-0.0360, grad=0.0000), Var(v=0.0627, grad=0.0000), Var(v=0.1811, grad=0.0000), Var(v=0.0055, grad=0.0000), Var(v=-0.0945, grad=0.0000), Var(v=-0.0113, grad=0.0000), Var(v=-0.1494, grad=0.0000), Var(v=-0.1049, grad=0.0000), Var(v=-0.2283, grad=0.0000), Var(v=0.1096, grad=0.0000), Var(v=-0.2114, grad=0.0000), Var(v=-0.0837, grad=0.0000), Var(v=0.0383, grad=0.0000), Var(v=0.0241, grad=0.0000), Var(v=-0.1216, grad=0.0000), Var(v=-0.0620, grad=0.0000), Var(v=0.1546, grad=0.0000), Var(v=-0.1381, grad=0.0000), Var(v=0.1335, grad=0.0000), Var(v=-0.0599, grad=0.0000), Var(v=0.0288, grad=0.0000), Var(v=0.0534, grad=0.0000), Var(v=-0.0990, grad=0.0000), Var(v=0.0911, grad=0.0000), Var(v=-0.1124, grad=0.0000), Var(v=0.0302, grad=0.0000), Var(v=0.0048, grad=0.0000), Var(v=0.0371, grad=0.0000), Var(v=0.0046, grad=0.0000), Var(v=0.0372, grad=0.0000), Var(v=0.0375, grad=0.0000), Var(v=0.0019, grad=0.0000), Var(v=0.1216, grad=0.0000), Var(v=-0.0320, grad=0.0000), Var(v=-0.1257, grad=0.0000), Var(v=-0.1636, grad=0.0000), Var(v=0.0333, grad=0.0000)], [Var(v=0.0928, grad=0.0000), Var(v=0.0450, grad=0.0000), Var(v=-0.2179, grad=0.0000), Var(v=0.0389, grad=0.0000), Var(v=0.1113, grad=0.0000), Var(v=-0.0066, grad=0.0000), Var(v=0.0630, grad=0.0000), Var(v=0.0258, grad=0.0000), Var(v=-0.0449, grad=0.0000), Var(v=-0.0697, grad=0.0000), Var(v=-0.0685, grad=0.0000), Var(v=0.0208, grad=0.0000), Var(v=0.1085, grad=0.0000), Var(v=0.0297, grad=0.0000), Var(v=0.1481, grad=0.0000), Var(v=-0.0053, grad=0.0000), Var(v=0.1157, grad=0.0000), Var(v=0.0079, grad=0.0000), Var(v=-0.0646, grad=0.0000), Var(v=0.0020, grad=0.0000), Var(v=0.0469, grad=0.0000), Var(v=-0.1633, grad=0.0000), Var(v=0.1790, grad=0.0000), Var(v=0.0133, grad=0.0000), Var(v=-0.1474, grad=0.0000), Var(v=-0.0265, grad=0.0000), Var(v=-0.1486, grad=0.0000), Var(v=0.0127, grad=0.0000), Var(v=0.1152, grad=0.0000), Var(v=0.0173, grad=0.0000), Var(v=-0.0433, grad=0.0000), Var(v=-0.0603, grad=0.0000), Var(v=0.0793, grad=0.0000), Var(v=0.1242, grad=0.0000), Var(v=0.0079, grad=0.0000), Var(v=-0.1331, grad=0.0000), Var(v=-0.0176, grad=0.0000), Var(v=0.2073, grad=0.0000), Var(v=-0.1025, grad=0.0000), Var(v=-0.0077, grad=0.0000), Var(v=0.0662, grad=0.0000), Var(v=0.1535, grad=0.0000), Var(v=0.0145, grad=0.0000), Var(v=0.0679, grad=0.0000), Var(v=-0.1390, grad=0.0000), Var(v=-0.0778, grad=0.0000), Var(v=0.0377, grad=0.0000), Var(v=0.0378, grad=0.0000), Var(v=0.0898, grad=0.0000), Var(v=-0.0604, grad=0.0000)], [Var(v=-0.0691, grad=0.0000), Var(v=-0.2256, grad=0.0000), Var(v=0.0354, grad=0.0000), Var(v=-0.0757, grad=0.0000), Var(v=-0.1173, grad=0.0000), Var(v=0.0311, grad=0.0000), Var(v=-0.0817, grad=0.0000), Var(v=-0.0061, grad=0.0000), Var(v=0.0004, grad=0.0000), Var(v=0.0808, grad=0.0000), Var(v=-0.0961, grad=0.0000), Var(v=0.0217, grad=0.0000), Var(v=0.1425, grad=0.0000), Var(v=-0.1454, grad=0.0000), Var(v=-0.1183, grad=0.0000), Var(v=0.0390, grad=0.0000), Var(v=0.1028, grad=0.0000), Var(v=0.0007, grad=0.0000), Var(v=-0.1319, grad=0.0000), Var(v=0.1592, grad=0.0000), Var(v=0.0141, grad=0.0000), Var(v=0.0317, grad=0.0000), Var(v=-0.0738, grad=0.0000), Var(v=0.0268, grad=0.0000), Var(v=0.1002, grad=0.0000), Var(v=-0.0454, grad=0.0000), Var(v=-0.0553, grad=0.0000), Var(v=-0.1261, grad=0.0000), Var(v=0.0262, grad=0.0000), Var(v=-0.1149, grad=0.0000), Var(v=0.0042, grad=0.0000), Var(v=-0.0455, grad=0.0000), Var(v=-0.2155, grad=0.0000), Var(v=0.1326, grad=0.0000), Var(v=0.0098, grad=0.0000), Var(v=0.0175, grad=0.0000), Var(v=-0.1357, grad=0.0000), Var(v=0.0077, grad=0.0000), Var(v=0.0838, grad=0.0000), Var(v=-0.1161, grad=0.0000), Var(v=-0.0175, grad=0.0000), Var(v=-0.1210, grad=0.0000), Var(v=0.0705, grad=0.0000), Var(v=0.0875, grad=0.0000), Var(v=0.1083, grad=0.0000), Var(v=-0.0585, grad=0.0000), Var(v=0.1497, grad=0.0000), Var(v=-0.0380, grad=0.0000), Var(v=0.1667, grad=0.0000), Var(v=0.0439, grad=0.0000)], [Var(v=-0.0705, grad=0.0000), Var(v=0.1076, grad=0.0000), Var(v=0.0090, grad=0.0000), Var(v=0.0933, grad=0.0000), Var(v=0.0264, grad=0.0000), Var(v=0.1047, grad=0.0000), Var(v=0.0852, grad=0.0000), Var(v=0.0351, grad=0.0000), Var(v=-0.0016, grad=0.0000), Var(v=0.1529, grad=0.0000), Var(v=0.0525, grad=0.0000), Var(v=-0.0975, grad=0.0000), Var(v=-0.0415, grad=0.0000), Var(v=-0.0578, grad=0.0000), Var(v=-0.0433, grad=0.0000), Var(v=-0.0020, grad=0.0000), Var(v=0.1206, grad=0.0000), Var(v=-0.0003, grad=0.0000), Var(v=-0.0446, grad=0.0000), Var(v=-0.2148, grad=0.0000), Var(v=0.0267, grad=0.0000), Var(v=0.1061, grad=0.0000), Var(v=0.0266, grad=0.0000), Var(v=0.0786, grad=0.0000), Var(v=-0.1724, grad=0.0000), Var(v=-0.1608, grad=0.0000), Var(v=0.0494, grad=0.0000), Var(v=-0.0479, grad=0.0000), Var(v=-0.0381, grad=0.0000), Var(v=-0.0264, grad=0.0000), Var(v=0.0292, grad=0.0000), Var(v=-0.2154, grad=0.0000), Var(v=0.0132, grad=0.0000), Var(v=0.1219, grad=0.0000), Var(v=0.0226, grad=0.0000), Var(v=-0.0729, grad=0.0000), Var(v=0.0703, grad=0.0000), Var(v=-0.0225, grad=0.0000), Var(v=-0.0218, grad=0.0000), Var(v=-0.0774, grad=0.0000), Var(v=0.1195, grad=0.0000), Var(v=0.1038, grad=0.0000), Var(v=-0.0412, grad=0.0000), Var(v=-0.0082, grad=0.0000), Var(v=0.0417, grad=0.0000), Var(v=-0.0709, grad=0.0000), Var(v=0.0060, grad=0.0000), Var(v=-0.0553, grad=0.0000), Var(v=0.0200, grad=0.0000), Var(v=0.0211, grad=0.0000)], [Var(v=-0.0520, grad=0.0000), Var(v=-0.1404, grad=0.0000), Var(v=0.0480, grad=0.0000), Var(v=-0.1845, grad=0.0000), Var(v=0.0589, grad=0.0000), Var(v=-0.0530, grad=0.0000), Var(v=-0.0041, grad=0.0000), Var(v=0.0083, grad=0.0000), Var(v=-0.0150, grad=0.0000), Var(v=-0.0491, grad=0.0000), Var(v=0.1347, grad=0.0000), Var(v=-0.0475, grad=0.0000), Var(v=0.0744, grad=0.0000), Var(v=0.1985, grad=0.0000), Var(v=0.1736, grad=0.0000), Var(v=0.0555, grad=0.0000), Var(v=0.0756, grad=0.0000), Var(v=-0.1431, grad=0.0000), Var(v=0.0572, grad=0.0000), Var(v=-0.0068, grad=0.0000), Var(v=0.0848, grad=0.0000), Var(v=0.0437, grad=0.0000), Var(v=0.0194, grad=0.0000), Var(v=0.0525, grad=0.0000), Var(v=-0.0295, grad=0.0000), Var(v=-0.1381, grad=0.0000), Var(v=-0.0098, grad=0.0000), Var(v=-0.0299, grad=0.0000), Var(v=-0.1061, grad=0.0000), Var(v=-0.0988, grad=0.0000), Var(v=-0.0597, grad=0.0000), Var(v=0.1287, grad=0.0000), Var(v=0.1162, grad=0.0000), Var(v=0.1105, grad=0.0000), Var(v=0.1783, grad=0.0000), Var(v=0.0267, grad=0.0000), Var(v=0.0459, grad=0.0000), Var(v=-0.0244, grad=0.0000), Var(v=0.0263, grad=0.0000), Var(v=-0.1805, grad=0.0000), Var(v=-0.0762, grad=0.0000), Var(v=0.1147, grad=0.0000), Var(v=-0.0283, grad=0.0000), Var(v=-0.2016, grad=0.0000), Var(v=0.0632, grad=0.0000), Var(v=0.1268, grad=0.0000), Var(v=-0.0642, grad=0.0000), Var(v=0.0718, grad=0.0000), Var(v=0.0182, grad=0.0000), Var(v=0.0158, grad=0.0000)], [Var(v=-0.0980, grad=0.0000), Var(v=0.2393, grad=0.0000), Var(v=-0.0376, grad=0.0000), Var(v=0.0504, grad=0.0000), Var(v=-0.2026, grad=0.0000), Var(v=0.1441, grad=0.0000), Var(v=-0.0539, grad=0.0000), Var(v=0.1183, grad=0.0000), Var(v=0.0041, grad=0.0000), Var(v=-0.0283, grad=0.0000), Var(v=-0.0683, grad=0.0000), Var(v=0.0227, grad=0.0000), Var(v=-0.0601, grad=0.0000), Var(v=0.0853, grad=0.0000), Var(v=-0.0053, grad=0.0000), Var(v=-0.0306, grad=0.0000), Var(v=-0.1458, grad=0.0000), Var(v=-0.0360, grad=0.0000), Var(v=-0.0853, grad=0.0000), Var(v=-0.0603, grad=0.0000), Var(v=0.0623, grad=0.0000), Var(v=-0.0806, grad=0.0000), Var(v=-0.1422, grad=0.0000), Var(v=-0.1307, grad=0.0000), Var(v=-0.1552, grad=0.0000), Var(v=-0.0904, grad=0.0000), Var(v=-0.0597, grad=0.0000), Var(v=-0.1896, grad=0.0000), Var(v=0.0715, grad=0.0000), Var(v=0.1101, grad=0.0000), Var(v=0.0361, grad=0.0000), Var(v=-0.1109, grad=0.0000), Var(v=-0.0306, grad=0.0000), Var(v=0.0846, grad=0.0000), Var(v=0.0117, grad=0.0000), Var(v=-0.1056, grad=0.0000), Var(v=-0.0437, grad=0.0000), Var(v=-0.0173, grad=0.0000), Var(v=0.0642, grad=0.0000), Var(v=-0.0385, grad=0.0000), Var(v=0.1043, grad=0.0000), Var(v=-0.1284, grad=0.0000), Var(v=0.1843, grad=0.0000), Var(v=0.1769, grad=0.0000), Var(v=0.1634, grad=0.0000), Var(v=0.1078, grad=0.0000), Var(v=0.1239, grad=0.0000), Var(v=-0.1447, grad=0.0000), Var(v=-0.0350, grad=0.0000), Var(v=-0.0107, grad=0.0000)], [Var(v=-0.0795, grad=0.0000), Var(v=-0.2219, grad=0.0000), Var(v=0.1430, grad=0.0000), Var(v=-0.0663, grad=0.0000), Var(v=0.2075, grad=0.0000), Var(v=-0.0064, grad=0.0000), Var(v=-0.0415, grad=0.0000), Var(v=0.0205, grad=0.0000), Var(v=-0.0158, grad=0.0000), Var(v=-0.0910, grad=0.0000), Var(v=-0.1394, grad=0.0000), Var(v=0.1515, grad=0.0000), Var(v=-0.1110, grad=0.0000), Var(v=0.1206, grad=0.0000), Var(v=-0.0600, grad=0.0000), Var(v=0.0870, grad=0.0000), Var(v=-0.1673, grad=0.0000), Var(v=-0.1159, grad=0.0000), Var(v=0.1671, grad=0.0000), Var(v=-0.0764, grad=0.0000), Var(v=0.1763, grad=0.0000), Var(v=0.0856, grad=0.0000), Var(v=0.0587, grad=0.0000), Var(v=0.0572, grad=0.0000), Var(v=0.1165, grad=0.0000), Var(v=0.0509, grad=0.0000), Var(v=0.0474, grad=0.0000), Var(v=-0.0012, grad=0.0000), Var(v=0.0915, grad=0.0000), Var(v=-0.0015, grad=0.0000), Var(v=-0.1006, grad=0.0000), Var(v=-0.1111, grad=0.0000), Var(v=-0.1479, grad=0.0000), Var(v=-0.1197, grad=0.0000), Var(v=0.0436, grad=0.0000), Var(v=-0.0062, grad=0.0000), Var(v=0.0389, grad=0.0000), Var(v=-0.1168, grad=0.0000), Var(v=0.1300, grad=0.0000), Var(v=0.0299, grad=0.0000), Var(v=0.0913, grad=0.0000), Var(v=0.1020, grad=0.0000), Var(v=0.0024, grad=0.0000), Var(v=-0.1494, grad=0.0000), Var(v=0.0071, grad=0.0000), Var(v=-0.0044, grad=0.0000), Var(v=0.0427, grad=0.0000), Var(v=0.0523, grad=0.0000), Var(v=-0.0360, grad=0.0000), Var(v=0.0925, grad=0.0000)], [Var(v=0.0360, grad=0.0000), Var(v=-0.0872, grad=0.0000), Var(v=0.0844, grad=0.0000), Var(v=-0.0483, grad=0.0000), Var(v=0.2329, grad=0.0000), Var(v=0.1712, grad=0.0000), Var(v=0.0573, grad=0.0000), Var(v=-0.0471, grad=0.0000), Var(v=0.1333, grad=0.0000), Var(v=-0.2557, grad=0.0000), Var(v=-0.0678, grad=0.0000), Var(v=0.1786, grad=0.0000), Var(v=-0.0824, grad=0.0000), Var(v=-0.0995, grad=0.0000), Var(v=-0.0103, grad=0.0000), Var(v=-0.0275, grad=0.0000), Var(v=-0.0375, grad=0.0000), Var(v=0.1337, grad=0.0000), Var(v=-0.0535, grad=0.0000), Var(v=0.0802, grad=0.0000), Var(v=-0.1159, grad=0.0000), Var(v=-0.0804, grad=0.0000), Var(v=-0.0580, grad=0.0000), Var(v=-0.1448, grad=0.0000), Var(v=0.0110, grad=0.0000), Var(v=0.1639, grad=0.0000), Var(v=-0.0498, grad=0.0000), Var(v=0.1139, grad=0.0000), Var(v=-0.0194, grad=0.0000), Var(v=-0.0358, grad=0.0000), Var(v=0.0035, grad=0.0000), Var(v=0.0584, grad=0.0000), Var(v=0.0052, grad=0.0000), Var(v=-0.2370, grad=0.0000), Var(v=0.2296, grad=0.0000), Var(v=0.0426, grad=0.0000), Var(v=-0.1402, grad=0.0000), Var(v=-0.1375, grad=0.0000), Var(v=0.0823, grad=0.0000), Var(v=-0.0594, grad=0.0000), Var(v=-0.1271, grad=0.0000), Var(v=-0.0090, grad=0.0000), Var(v=0.0291, grad=0.0000), Var(v=0.0410, grad=0.0000), Var(v=-0.0237, grad=0.0000), Var(v=-0.1335, grad=0.0000), Var(v=0.1442, grad=0.0000), Var(v=0.0152, grad=0.0000), Var(v=-0.0495, grad=0.0000), Var(v=0.0113, grad=0.0000)], [Var(v=-0.0230, grad=0.0000), Var(v=-0.0974, grad=0.0000), Var(v=0.0363, grad=0.0000), Var(v=-0.0911, grad=0.0000), Var(v=-0.0147, grad=0.0000), Var(v=0.0216, grad=0.0000), Var(v=-0.0704, grad=0.0000), Var(v=-0.1212, grad=0.0000), Var(v=-0.0641, grad=0.0000), Var(v=-0.1011, grad=0.0000), Var(v=-0.2170, grad=0.0000), Var(v=-0.0477, grad=0.0000), Var(v=-0.0769, grad=0.0000), Var(v=-0.1381, grad=0.0000), Var(v=0.0459, grad=0.0000), Var(v=-0.1407, grad=0.0000), Var(v=0.0625, grad=0.0000), Var(v=0.0976, grad=0.0000), Var(v=0.1410, grad=0.0000), Var(v=0.0494, grad=0.0000), Var(v=-0.0647, grad=0.0000), Var(v=0.0599, grad=0.0000), Var(v=-0.1083, grad=0.0000), Var(v=0.0549, grad=0.0000), Var(v=0.1428, grad=0.0000), Var(v=-0.0594, grad=0.0000), Var(v=-0.0057, grad=0.0000), Var(v=0.1400, grad=0.0000), Var(v=-0.0099, grad=0.0000), Var(v=0.0514, grad=0.0000), Var(v=-0.1530, grad=0.0000), Var(v=0.1448, grad=0.0000), Var(v=0.0045, grad=0.0000), Var(v=0.1521, grad=0.0000), Var(v=-0.0457, grad=0.0000), Var(v=0.0924, grad=0.0000), Var(v=0.0095, grad=0.0000), Var(v=-0.0012, grad=0.0000), Var(v=0.2218, grad=0.0000), Var(v=0.0399, grad=0.0000), Var(v=0.0732, grad=0.0000), Var(v=-0.1548, grad=0.0000), Var(v=-0.0953, grad=0.0000), Var(v=-0.1056, grad=0.0000), Var(v=-0.0484, grad=0.0000), Var(v=-0.1120, grad=0.0000), Var(v=-0.0173, grad=0.0000), Var(v=0.0895, grad=0.0000), Var(v=0.0286, grad=0.0000), Var(v=0.0044, grad=0.0000)], [Var(v=-0.0185, grad=0.0000), Var(v=0.0904, grad=0.0000), Var(v=0.1375, grad=0.0000), Var(v=-0.0327, grad=0.0000), Var(v=0.0297, grad=0.0000), Var(v=0.0270, grad=0.0000), Var(v=0.2811, grad=0.0000), Var(v=0.1674, grad=0.0000), Var(v=0.0065, grad=0.0000), Var(v=-0.0045, grad=0.0000), Var(v=-0.0988, grad=0.0000), Var(v=-0.3583, grad=0.0000), Var(v=0.0038, grad=0.0000), Var(v=0.0126, grad=0.0000), Var(v=-0.1645, grad=0.0000), Var(v=0.0446, grad=0.0000), Var(v=-0.0258, grad=0.0000), Var(v=-0.1593, grad=0.0000), Var(v=-0.1192, grad=0.0000), Var(v=-0.0009, grad=0.0000), Var(v=-0.0923, grad=0.0000), Var(v=0.2304, grad=0.0000), Var(v=0.1234, grad=0.0000), Var(v=0.0919, grad=0.0000), Var(v=0.0956, grad=0.0000), Var(v=-0.1140, grad=0.0000), Var(v=-0.1327, grad=0.0000), Var(v=-0.1356, grad=0.0000), Var(v=-0.0018, grad=0.0000), Var(v=0.1747, grad=0.0000), Var(v=0.1498, grad=0.0000), Var(v=-0.0247, grad=0.0000), Var(v=0.0976, grad=0.0000), Var(v=0.2138, grad=0.0000), Var(v=-0.0298, grad=0.0000), Var(v=0.0858, grad=0.0000), Var(v=-0.0518, grad=0.0000), Var(v=-0.0629, grad=0.0000), Var(v=-0.0960, grad=0.0000), Var(v=-0.0173, grad=0.0000), Var(v=0.0789, grad=0.0000), Var(v=-0.0126, grad=0.0000), Var(v=-0.0424, grad=0.0000), Var(v=-0.0332, grad=0.0000), Var(v=0.0017, grad=0.0000), Var(v=0.0388, grad=0.0000), Var(v=-0.1861, grad=0.0000), Var(v=0.0564, grad=0.0000), Var(v=-0.1230, grad=0.0000), Var(v=-0.0993, grad=0.0000)], [Var(v=-0.0680, grad=0.0000), Var(v=0.0613, grad=0.0000), Var(v=0.0589, grad=0.0000), Var(v=0.2238, grad=0.0000), Var(v=-0.0179, grad=0.0000), Var(v=-0.0599, grad=0.0000), Var(v=-0.0493, grad=0.0000), Var(v=0.0222, grad=0.0000), Var(v=-0.0795, grad=0.0000), Var(v=0.0636, grad=0.0000), Var(v=-0.0619, grad=0.0000), Var(v=-0.0031, grad=0.0000), Var(v=0.0873, grad=0.0000), Var(v=0.1240, grad=0.0000), Var(v=-0.0777, grad=0.0000), Var(v=0.0177, grad=0.0000), Var(v=0.0226, grad=0.0000), Var(v=-0.1396, grad=0.0000), Var(v=0.1780, grad=0.0000), Var(v=0.0495, grad=0.0000), Var(v=-0.0366, grad=0.0000), Var(v=0.0979, grad=0.0000), Var(v=0.0299, grad=0.0000), Var(v=0.0489, grad=0.0000), Var(v=-0.0806, grad=0.0000), Var(v=-0.1319, grad=0.0000), Var(v=0.1019, grad=0.0000), Var(v=0.0998, grad=0.0000), Var(v=0.0368, grad=0.0000), Var(v=0.1133, grad=0.0000), Var(v=0.0127, grad=0.0000), Var(v=0.1236, grad=0.0000), Var(v=-0.0862, grad=0.0000), Var(v=-0.0992, grad=0.0000), Var(v=0.1581, grad=0.0000), Var(v=-0.0984, grad=0.0000), Var(v=0.0457, grad=0.0000), Var(v=0.2303, grad=0.0000), Var(v=-0.0273, grad=0.0000), Var(v=0.0196, grad=0.0000), Var(v=0.0721, grad=0.0000), Var(v=-0.0724, grad=0.0000), Var(v=0.0981, grad=0.0000), Var(v=0.0664, grad=0.0000), Var(v=0.0816, grad=0.0000), Var(v=-0.0720, grad=0.0000), Var(v=-0.0270, grad=0.0000), Var(v=0.0303, grad=0.0000), Var(v=0.0388, grad=0.0000), Var(v=-0.1155, grad=0.0000)], [Var(v=0.1001, grad=0.0000), Var(v=-0.0586, grad=0.0000), Var(v=-0.0263, grad=0.0000), Var(v=0.0554, grad=0.0000), Var(v=-0.0463, grad=0.0000), Var(v=0.1039, grad=0.0000), Var(v=-0.0805, grad=0.0000), Var(v=-0.0836, grad=0.0000), Var(v=0.0150, grad=0.0000), Var(v=0.0194, grad=0.0000), Var(v=-0.0841, grad=0.0000), Var(v=0.2141, grad=0.0000), Var(v=-0.0417, grad=0.0000), Var(v=0.1512, grad=0.0000), Var(v=0.1732, grad=0.0000), Var(v=0.1146, grad=0.0000), Var(v=0.0235, grad=0.0000), Var(v=-0.0735, grad=0.0000), Var(v=-0.0125, grad=0.0000), Var(v=-0.1590, grad=0.0000), Var(v=-0.0597, grad=0.0000), Var(v=-0.0559, grad=0.0000), Var(v=0.0912, grad=0.0000), Var(v=0.1122, grad=0.0000), Var(v=-0.0250, grad=0.0000), Var(v=-0.0161, grad=0.0000), Var(v=0.1177, grad=0.0000), Var(v=0.1163, grad=0.0000), Var(v=0.1896, grad=0.0000), Var(v=-0.0351, grad=0.0000), Var(v=0.0085, grad=0.0000), Var(v=0.1746, grad=0.0000), Var(v=-0.0271, grad=0.0000), Var(v=-0.0848, grad=0.0000), Var(v=-0.0202, grad=0.0000), Var(v=-0.0666, grad=0.0000), Var(v=0.0525, grad=0.0000), Var(v=-0.0695, grad=0.0000), Var(v=0.1094, grad=0.0000), Var(v=0.0287, grad=0.0000), Var(v=-0.0192, grad=0.0000), Var(v=-0.0388, grad=0.0000), Var(v=-0.0459, grad=0.0000), Var(v=0.0071, grad=0.0000), Var(v=0.0091, grad=0.0000), Var(v=0.0014, grad=0.0000), Var(v=0.0843, grad=0.0000), Var(v=-0.2273, grad=0.0000), Var(v=0.0313, grad=0.0000), Var(v=0.0226, grad=0.0000)], [Var(v=-0.1682, grad=0.0000), Var(v=-0.1121, grad=0.0000), Var(v=-0.0142, grad=0.0000), Var(v=-0.0005, grad=0.0000), Var(v=-0.1236, grad=0.0000), Var(v=-0.1059, grad=0.0000), Var(v=-0.0699, grad=0.0000), Var(v=-0.1441, grad=0.0000), Var(v=0.0391, grad=0.0000), Var(v=-0.1460, grad=0.0000), Var(v=0.0377, grad=0.0000), Var(v=0.0293, grad=0.0000), Var(v=0.1150, grad=0.0000), Var(v=0.0328, grad=0.0000), Var(v=-0.1097, grad=0.0000), Var(v=-0.0012, grad=0.0000), Var(v=-0.1222, grad=0.0000), Var(v=-0.0782, grad=0.0000), Var(v=0.0535, grad=0.0000), Var(v=-0.0098, grad=0.0000), Var(v=0.0153, grad=0.0000), Var(v=0.0509, grad=0.0000), Var(v=-0.1471, grad=0.0000), Var(v=-0.0561, grad=0.0000), Var(v=0.1645, grad=0.0000), Var(v=0.0443, grad=0.0000), Var(v=0.1114, grad=0.0000), Var(v=-0.1016, grad=0.0000), Var(v=-0.0728, grad=0.0000), Var(v=0.0800, grad=0.0000), Var(v=0.0817, grad=0.0000), Var(v=0.0600, grad=0.0000), Var(v=-0.0198, grad=0.0000), Var(v=0.0451, grad=0.0000), Var(v=-0.0342, grad=0.0000), Var(v=-0.1045, grad=0.0000), Var(v=-0.0027, grad=0.0000), Var(v=0.0809, grad=0.0000), Var(v=0.0238, grad=0.0000), Var(v=0.0925, grad=0.0000), Var(v=0.0710, grad=0.0000), Var(v=0.0058, grad=0.0000), Var(v=0.0723, grad=0.0000), Var(v=0.0275, grad=0.0000), Var(v=-0.0573, grad=0.0000), Var(v=0.1107, grad=0.0000), Var(v=-0.0611, grad=0.0000), Var(v=0.0160, grad=0.0000), Var(v=0.1622, grad=0.0000), Var(v=-0.0427, grad=0.0000)], [Var(v=0.0430, grad=0.0000), Var(v=-0.0656, grad=0.0000), Var(v=0.0600, grad=0.0000), Var(v=-0.1209, grad=0.0000), Var(v=0.0129, grad=0.0000), Var(v=-0.0592, grad=0.0000), Var(v=0.0400, grad=0.0000), Var(v=-0.0171, grad=0.0000), Var(v=-0.1256, grad=0.0000), Var(v=-0.0797, grad=0.0000), Var(v=0.0057, grad=0.0000), Var(v=0.0897, grad=0.0000), Var(v=-0.0859, grad=0.0000), Var(v=0.1830, grad=0.0000), Var(v=-0.1157, grad=0.0000), Var(v=0.1066, grad=0.0000), Var(v=-0.0889, grad=0.0000), Var(v=0.0805, grad=0.0000), Var(v=-0.0596, grad=0.0000), Var(v=0.0495, grad=0.0000), Var(v=-0.1089, grad=0.0000), Var(v=0.0261, grad=0.0000), Var(v=-0.1168, grad=0.0000), Var(v=0.0589, grad=0.0000), Var(v=-0.0218, grad=0.0000), Var(v=0.0482, grad=0.0000), Var(v=-0.1228, grad=0.0000), Var(v=-0.0760, grad=0.0000), Var(v=0.0733, grad=0.0000), Var(v=-0.0073, grad=0.0000), Var(v=0.0767, grad=0.0000), Var(v=-0.0488, grad=0.0000), Var(v=0.0216, grad=0.0000), Var(v=-0.1021, grad=0.0000), Var(v=-0.1707, grad=0.0000), Var(v=0.1442, grad=0.0000), Var(v=-0.1705, grad=0.0000), Var(v=-0.0926, grad=0.0000), Var(v=0.1652, grad=0.0000), Var(v=-0.1452, grad=0.0000), Var(v=0.0562, grad=0.0000), Var(v=0.0124, grad=0.0000), Var(v=-0.0935, grad=0.0000), Var(v=0.0673, grad=0.0000), Var(v=-0.0336, grad=0.0000), Var(v=-0.0298, grad=0.0000), Var(v=0.0311, grad=0.0000), Var(v=0.0384, grad=0.0000), Var(v=-0.0260, grad=0.0000), Var(v=-0.0669, grad=0.0000)], [Var(v=0.2684, grad=0.0000), Var(v=0.1023, grad=0.0000), Var(v=0.0080, grad=0.0000), Var(v=-0.1075, grad=0.0000), Var(v=0.1242, grad=0.0000), Var(v=-0.0436, grad=0.0000), Var(v=-0.0564, grad=0.0000), Var(v=-0.1048, grad=0.0000), Var(v=0.2006, grad=0.0000), Var(v=-0.1082, grad=0.0000), Var(v=-0.0278, grad=0.0000), Var(v=0.1813, grad=0.0000), Var(v=-0.0246, grad=0.0000), Var(v=0.0153, grad=0.0000), Var(v=0.2094, grad=0.0000), Var(v=0.0068, grad=0.0000), Var(v=0.0519, grad=0.0000), Var(v=0.0068, grad=0.0000), Var(v=0.1407, grad=0.0000), Var(v=-0.2884, grad=0.0000), Var(v=0.2353, grad=0.0000), Var(v=0.0601, grad=0.0000), Var(v=0.1291, grad=0.0000), Var(v=-0.0513, grad=0.0000), Var(v=-0.1170, grad=0.0000), Var(v=0.0141, grad=0.0000), Var(v=0.0304, grad=0.0000), Var(v=0.0139, grad=0.0000), Var(v=-0.0886, grad=0.0000), Var(v=0.0091, grad=0.0000), Var(v=0.0241, grad=0.0000), Var(v=-0.0375, grad=0.0000), Var(v=-0.0049, grad=0.0000), Var(v=-0.0360, grad=0.0000), Var(v=-0.0639, grad=0.0000), Var(v=0.2345, grad=0.0000), Var(v=0.0406, grad=0.0000), Var(v=-0.0911, grad=0.0000), Var(v=-0.0315, grad=0.0000), Var(v=0.0355, grad=0.0000), Var(v=0.0027, grad=0.0000), Var(v=0.0334, grad=0.0000), Var(v=-0.0117, grad=0.0000), Var(v=-0.1197, grad=0.0000), Var(v=0.1534, grad=0.0000), Var(v=-0.0844, grad=0.0000), Var(v=0.1814, grad=0.0000), Var(v=-0.0674, grad=0.0000), Var(v=-0.0882, grad=0.0000), Var(v=0.0766, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
      "Layer 2 \n",
      " Weights: [[Var(v=0.1259, grad=0.0000)], [Var(v=0.1001, grad=0.0000)], [Var(v=0.0469, grad=0.0000)], [Var(v=0.0235, grad=0.0000)], [Var(v=-0.1436, grad=0.0000)], [Var(v=-0.0939, grad=0.0000)], [Var(v=-0.0325, grad=0.0000)], [Var(v=0.0222, grad=0.0000)], [Var(v=-0.0984, grad=0.0000)], [Var(v=0.0655, grad=0.0000)], [Var(v=0.0555, grad=0.0000)], [Var(v=0.0674, grad=0.0000)], [Var(v=-0.0067, grad=0.0000)], [Var(v=-0.0809, grad=0.0000)], [Var(v=0.0027, grad=0.0000)], [Var(v=-0.1017, grad=0.0000)], [Var(v=0.1347, grad=0.0000)], [Var(v=0.1161, grad=0.0000)], [Var(v=-0.0783, grad=0.0000)], [Var(v=-0.0714, grad=0.0000)], [Var(v=0.0044, grad=0.0000)], [Var(v=0.0474, grad=0.0000)], [Var(v=0.0914, grad=0.0000)], [Var(v=-0.0201, grad=0.0000)], [Var(v=-0.0661, grad=0.0000)], [Var(v=0.0872, grad=0.0000)], [Var(v=-0.0453, grad=0.0000)], [Var(v=0.0057, grad=0.0000)], [Var(v=0.1406, grad=0.0000)], [Var(v=-0.0051, grad=0.0000)], [Var(v=-0.1653, grad=0.0000)], [Var(v=0.1622, grad=0.0000)], [Var(v=-0.0684, grad=0.0000)], [Var(v=0.0205, grad=0.0000)], [Var(v=0.0438, grad=0.0000)], [Var(v=-0.1548, grad=0.0000)], [Var(v=-0.1727, grad=0.0000)], [Var(v=0.0205, grad=0.0000)], [Var(v=-0.0046, grad=0.0000)], [Var(v=-0.0635, grad=0.0000)], [Var(v=0.0127, grad=0.0000)], [Var(v=-0.1018, grad=0.0000)], [Var(v=0.0273, grad=0.0000)], [Var(v=-0.0590, grad=0.0000)], [Var(v=-0.1019, grad=0.0000)], [Var(v=0.0644, grad=0.0000)], [Var(v=-0.0768, grad=0.0000)], [Var(v=-0.0693, grad=0.0000)], [Var(v=-0.2110, grad=0.0000)], [Var(v=-0.0403, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000)]\n",
      "\n",
      "Network after zeroing gradients:\n",
      "Layer 0 \n",
      " Weights: [[Var(v=0.0262, grad=0.0000), Var(v=0.0467, grad=0.0000), Var(v=-0.0064, grad=0.0000), Var(v=-0.0785, grad=0.0000), Var(v=-0.0141, grad=0.0000), Var(v=-0.1341, grad=0.0000), Var(v=0.0714, grad=0.0000), Var(v=-0.1879, grad=0.0000), Var(v=-0.0761, grad=0.0000), Var(v=0.0757, grad=0.0000), Var(v=-0.1493, grad=0.0000), Var(v=0.0084, grad=0.0000), Var(v=0.2500, grad=0.0000), Var(v=0.0600, grad=0.0000), Var(v=0.0567, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
      "Layer 1 \n",
      " Weights: [[Var(v=0.0958, grad=0.0000), Var(v=-0.1757, grad=0.0000), Var(v=-0.0777, grad=0.0000), Var(v=-0.1264, grad=0.0000), Var(v=0.0697, grad=0.0000), Var(v=0.0561, grad=0.0000), Var(v=-0.0201, grad=0.0000), Var(v=-0.0097, grad=0.0000), Var(v=0.0988, grad=0.0000), Var(v=0.0684, grad=0.0000), Var(v=0.0712, grad=0.0000), Var(v=-0.2111, grad=0.0000), Var(v=-0.0489, grad=0.0000), Var(v=-0.0360, grad=0.0000), Var(v=0.0627, grad=0.0000), Var(v=0.1811, grad=0.0000), Var(v=0.0055, grad=0.0000), Var(v=-0.0945, grad=0.0000), Var(v=-0.0113, grad=0.0000), Var(v=-0.1494, grad=0.0000), Var(v=-0.1049, grad=0.0000), Var(v=-0.2283, grad=0.0000), Var(v=0.1096, grad=0.0000), Var(v=-0.2114, grad=0.0000), Var(v=-0.0837, grad=0.0000), Var(v=0.0383, grad=0.0000), Var(v=0.0241, grad=0.0000), Var(v=-0.1216, grad=0.0000), Var(v=-0.0620, grad=0.0000), Var(v=0.1546, grad=0.0000), Var(v=-0.1381, grad=0.0000), Var(v=0.1335, grad=0.0000), Var(v=-0.0599, grad=0.0000), Var(v=0.0288, grad=0.0000), Var(v=0.0534, grad=0.0000), Var(v=-0.0990, grad=0.0000), Var(v=0.0911, grad=0.0000), Var(v=-0.1124, grad=0.0000), Var(v=0.0302, grad=0.0000), Var(v=0.0048, grad=0.0000), Var(v=0.0371, grad=0.0000), Var(v=0.0046, grad=0.0000), Var(v=0.0372, grad=0.0000), Var(v=0.0375, grad=0.0000), Var(v=0.0019, grad=0.0000), Var(v=0.1216, grad=0.0000), Var(v=-0.0320, grad=0.0000), Var(v=-0.1257, grad=0.0000), Var(v=-0.1636, grad=0.0000), Var(v=0.0333, grad=0.0000)], [Var(v=0.0928, grad=0.0000), Var(v=0.0450, grad=0.0000), Var(v=-0.2179, grad=0.0000), Var(v=0.0389, grad=0.0000), Var(v=0.1113, grad=0.0000), Var(v=-0.0066, grad=0.0000), Var(v=0.0630, grad=0.0000), Var(v=0.0258, grad=0.0000), Var(v=-0.0449, grad=0.0000), Var(v=-0.0697, grad=0.0000), Var(v=-0.0685, grad=0.0000), Var(v=0.0208, grad=0.0000), Var(v=0.1085, grad=0.0000), Var(v=0.0297, grad=0.0000), Var(v=0.1481, grad=0.0000), Var(v=-0.0053, grad=0.0000), Var(v=0.1157, grad=0.0000), Var(v=0.0079, grad=0.0000), Var(v=-0.0646, grad=0.0000), Var(v=0.0020, grad=0.0000), Var(v=0.0469, grad=0.0000), Var(v=-0.1633, grad=0.0000), Var(v=0.1790, grad=0.0000), Var(v=0.0133, grad=0.0000), Var(v=-0.1474, grad=0.0000), Var(v=-0.0265, grad=0.0000), Var(v=-0.1486, grad=0.0000), Var(v=0.0127, grad=0.0000), Var(v=0.1152, grad=0.0000), Var(v=0.0173, grad=0.0000), Var(v=-0.0433, grad=0.0000), Var(v=-0.0603, grad=0.0000), Var(v=0.0793, grad=0.0000), Var(v=0.1242, grad=0.0000), Var(v=0.0079, grad=0.0000), Var(v=-0.1331, grad=0.0000), Var(v=-0.0176, grad=0.0000), Var(v=0.2073, grad=0.0000), Var(v=-0.1025, grad=0.0000), Var(v=-0.0077, grad=0.0000), Var(v=0.0662, grad=0.0000), Var(v=0.1535, grad=0.0000), Var(v=0.0145, grad=0.0000), Var(v=0.0679, grad=0.0000), Var(v=-0.1390, grad=0.0000), Var(v=-0.0778, grad=0.0000), Var(v=0.0377, grad=0.0000), Var(v=0.0378, grad=0.0000), Var(v=0.0898, grad=0.0000), Var(v=-0.0604, grad=0.0000)], [Var(v=-0.0691, grad=0.0000), Var(v=-0.2256, grad=0.0000), Var(v=0.0354, grad=0.0000), Var(v=-0.0757, grad=0.0000), Var(v=-0.1173, grad=0.0000), Var(v=0.0311, grad=0.0000), Var(v=-0.0817, grad=0.0000), Var(v=-0.0061, grad=0.0000), Var(v=0.0004, grad=0.0000), Var(v=0.0808, grad=0.0000), Var(v=-0.0961, grad=0.0000), Var(v=0.0217, grad=0.0000), Var(v=0.1425, grad=0.0000), Var(v=-0.1454, grad=0.0000), Var(v=-0.1183, grad=0.0000), Var(v=0.0390, grad=0.0000), Var(v=0.1028, grad=0.0000), Var(v=0.0007, grad=0.0000), Var(v=-0.1319, grad=0.0000), Var(v=0.1592, grad=0.0000), Var(v=0.0141, grad=0.0000), Var(v=0.0317, grad=0.0000), Var(v=-0.0738, grad=0.0000), Var(v=0.0268, grad=0.0000), Var(v=0.1002, grad=0.0000), Var(v=-0.0454, grad=0.0000), Var(v=-0.0553, grad=0.0000), Var(v=-0.1261, grad=0.0000), Var(v=0.0262, grad=0.0000), Var(v=-0.1149, grad=0.0000), Var(v=0.0042, grad=0.0000), Var(v=-0.0455, grad=0.0000), Var(v=-0.2155, grad=0.0000), Var(v=0.1326, grad=0.0000), Var(v=0.0098, grad=0.0000), Var(v=0.0175, grad=0.0000), Var(v=-0.1357, grad=0.0000), Var(v=0.0077, grad=0.0000), Var(v=0.0838, grad=0.0000), Var(v=-0.1161, grad=0.0000), Var(v=-0.0175, grad=0.0000), Var(v=-0.1210, grad=0.0000), Var(v=0.0705, grad=0.0000), Var(v=0.0875, grad=0.0000), Var(v=0.1083, grad=0.0000), Var(v=-0.0585, grad=0.0000), Var(v=0.1497, grad=0.0000), Var(v=-0.0380, grad=0.0000), Var(v=0.1667, grad=0.0000), Var(v=0.0439, grad=0.0000)], [Var(v=-0.0705, grad=0.0000), Var(v=0.1076, grad=0.0000), Var(v=0.0090, grad=0.0000), Var(v=0.0933, grad=0.0000), Var(v=0.0264, grad=0.0000), Var(v=0.1047, grad=0.0000), Var(v=0.0852, grad=0.0000), Var(v=0.0351, grad=0.0000), Var(v=-0.0016, grad=0.0000), Var(v=0.1529, grad=0.0000), Var(v=0.0525, grad=0.0000), Var(v=-0.0975, grad=0.0000), Var(v=-0.0415, grad=0.0000), Var(v=-0.0578, grad=0.0000), Var(v=-0.0433, grad=0.0000), Var(v=-0.0020, grad=0.0000), Var(v=0.1206, grad=0.0000), Var(v=-0.0003, grad=0.0000), Var(v=-0.0446, grad=0.0000), Var(v=-0.2148, grad=0.0000), Var(v=0.0267, grad=0.0000), Var(v=0.1061, grad=0.0000), Var(v=0.0266, grad=0.0000), Var(v=0.0786, grad=0.0000), Var(v=-0.1724, grad=0.0000), Var(v=-0.1608, grad=0.0000), Var(v=0.0494, grad=0.0000), Var(v=-0.0479, grad=0.0000), Var(v=-0.0381, grad=0.0000), Var(v=-0.0264, grad=0.0000), Var(v=0.0292, grad=0.0000), Var(v=-0.2154, grad=0.0000), Var(v=0.0132, grad=0.0000), Var(v=0.1219, grad=0.0000), Var(v=0.0226, grad=0.0000), Var(v=-0.0729, grad=0.0000), Var(v=0.0703, grad=0.0000), Var(v=-0.0225, grad=0.0000), Var(v=-0.0218, grad=0.0000), Var(v=-0.0774, grad=0.0000), Var(v=0.1195, grad=0.0000), Var(v=0.1038, grad=0.0000), Var(v=-0.0412, grad=0.0000), Var(v=-0.0082, grad=0.0000), Var(v=0.0417, grad=0.0000), Var(v=-0.0709, grad=0.0000), Var(v=0.0060, grad=0.0000), Var(v=-0.0553, grad=0.0000), Var(v=0.0200, grad=0.0000), Var(v=0.0211, grad=0.0000)], [Var(v=-0.0520, grad=0.0000), Var(v=-0.1404, grad=0.0000), Var(v=0.0480, grad=0.0000), Var(v=-0.1845, grad=0.0000), Var(v=0.0589, grad=0.0000), Var(v=-0.0530, grad=0.0000), Var(v=-0.0041, grad=0.0000), Var(v=0.0083, grad=0.0000), Var(v=-0.0150, grad=0.0000), Var(v=-0.0491, grad=0.0000), Var(v=0.1347, grad=0.0000), Var(v=-0.0475, grad=0.0000), Var(v=0.0744, grad=0.0000), Var(v=0.1985, grad=0.0000), Var(v=0.1736, grad=0.0000), Var(v=0.0555, grad=0.0000), Var(v=0.0756, grad=0.0000), Var(v=-0.1431, grad=0.0000), Var(v=0.0572, grad=0.0000), Var(v=-0.0068, grad=0.0000), Var(v=0.0848, grad=0.0000), Var(v=0.0437, grad=0.0000), Var(v=0.0194, grad=0.0000), Var(v=0.0525, grad=0.0000), Var(v=-0.0295, grad=0.0000), Var(v=-0.1381, grad=0.0000), Var(v=-0.0098, grad=0.0000), Var(v=-0.0299, grad=0.0000), Var(v=-0.1061, grad=0.0000), Var(v=-0.0988, grad=0.0000), Var(v=-0.0597, grad=0.0000), Var(v=0.1287, grad=0.0000), Var(v=0.1162, grad=0.0000), Var(v=0.1105, grad=0.0000), Var(v=0.1783, grad=0.0000), Var(v=0.0267, grad=0.0000), Var(v=0.0459, grad=0.0000), Var(v=-0.0244, grad=0.0000), Var(v=0.0263, grad=0.0000), Var(v=-0.1805, grad=0.0000), Var(v=-0.0762, grad=0.0000), Var(v=0.1147, grad=0.0000), Var(v=-0.0283, grad=0.0000), Var(v=-0.2016, grad=0.0000), Var(v=0.0632, grad=0.0000), Var(v=0.1268, grad=0.0000), Var(v=-0.0642, grad=0.0000), Var(v=0.0718, grad=0.0000), Var(v=0.0182, grad=0.0000), Var(v=0.0158, grad=0.0000)], [Var(v=-0.0980, grad=0.0000), Var(v=0.2393, grad=0.0000), Var(v=-0.0376, grad=0.0000), Var(v=0.0504, grad=0.0000), Var(v=-0.2026, grad=0.0000), Var(v=0.1441, grad=0.0000), Var(v=-0.0539, grad=0.0000), Var(v=0.1183, grad=0.0000), Var(v=0.0041, grad=0.0000), Var(v=-0.0283, grad=0.0000), Var(v=-0.0683, grad=0.0000), Var(v=0.0227, grad=0.0000), Var(v=-0.0601, grad=0.0000), Var(v=0.0853, grad=0.0000), Var(v=-0.0053, grad=0.0000), Var(v=-0.0306, grad=0.0000), Var(v=-0.1458, grad=0.0000), Var(v=-0.0360, grad=0.0000), Var(v=-0.0853, grad=0.0000), Var(v=-0.0603, grad=0.0000), Var(v=0.0623, grad=0.0000), Var(v=-0.0806, grad=0.0000), Var(v=-0.1422, grad=0.0000), Var(v=-0.1307, grad=0.0000), Var(v=-0.1552, grad=0.0000), Var(v=-0.0904, grad=0.0000), Var(v=-0.0597, grad=0.0000), Var(v=-0.1896, grad=0.0000), Var(v=0.0715, grad=0.0000), Var(v=0.1101, grad=0.0000), Var(v=0.0361, grad=0.0000), Var(v=-0.1109, grad=0.0000), Var(v=-0.0306, grad=0.0000), Var(v=0.0846, grad=0.0000), Var(v=0.0117, grad=0.0000), Var(v=-0.1056, grad=0.0000), Var(v=-0.0437, grad=0.0000), Var(v=-0.0173, grad=0.0000), Var(v=0.0642, grad=0.0000), Var(v=-0.0385, grad=0.0000), Var(v=0.1043, grad=0.0000), Var(v=-0.1284, grad=0.0000), Var(v=0.1843, grad=0.0000), Var(v=0.1769, grad=0.0000), Var(v=0.1634, grad=0.0000), Var(v=0.1078, grad=0.0000), Var(v=0.1239, grad=0.0000), Var(v=-0.1447, grad=0.0000), Var(v=-0.0350, grad=0.0000), Var(v=-0.0107, grad=0.0000)], [Var(v=-0.0795, grad=0.0000), Var(v=-0.2219, grad=0.0000), Var(v=0.1430, grad=0.0000), Var(v=-0.0663, grad=0.0000), Var(v=0.2075, grad=0.0000), Var(v=-0.0064, grad=0.0000), Var(v=-0.0415, grad=0.0000), Var(v=0.0205, grad=0.0000), Var(v=-0.0158, grad=0.0000), Var(v=-0.0910, grad=0.0000), Var(v=-0.1394, grad=0.0000), Var(v=0.1515, grad=0.0000), Var(v=-0.1110, grad=0.0000), Var(v=0.1206, grad=0.0000), Var(v=-0.0600, grad=0.0000), Var(v=0.0870, grad=0.0000), Var(v=-0.1673, grad=0.0000), Var(v=-0.1159, grad=0.0000), Var(v=0.1671, grad=0.0000), Var(v=-0.0764, grad=0.0000), Var(v=0.1763, grad=0.0000), Var(v=0.0856, grad=0.0000), Var(v=0.0587, grad=0.0000), Var(v=0.0572, grad=0.0000), Var(v=0.1165, grad=0.0000), Var(v=0.0509, grad=0.0000), Var(v=0.0474, grad=0.0000), Var(v=-0.0012, grad=0.0000), Var(v=0.0915, grad=0.0000), Var(v=-0.0015, grad=0.0000), Var(v=-0.1006, grad=0.0000), Var(v=-0.1111, grad=0.0000), Var(v=-0.1479, grad=0.0000), Var(v=-0.1197, grad=0.0000), Var(v=0.0436, grad=0.0000), Var(v=-0.0062, grad=0.0000), Var(v=0.0389, grad=0.0000), Var(v=-0.1168, grad=0.0000), Var(v=0.1300, grad=0.0000), Var(v=0.0299, grad=0.0000), Var(v=0.0913, grad=0.0000), Var(v=0.1020, grad=0.0000), Var(v=0.0024, grad=0.0000), Var(v=-0.1494, grad=0.0000), Var(v=0.0071, grad=0.0000), Var(v=-0.0044, grad=0.0000), Var(v=0.0427, grad=0.0000), Var(v=0.0523, grad=0.0000), Var(v=-0.0360, grad=0.0000), Var(v=0.0925, grad=0.0000)], [Var(v=0.0360, grad=0.0000), Var(v=-0.0872, grad=0.0000), Var(v=0.0844, grad=0.0000), Var(v=-0.0483, grad=0.0000), Var(v=0.2329, grad=0.0000), Var(v=0.1712, grad=0.0000), Var(v=0.0573, grad=0.0000), Var(v=-0.0471, grad=0.0000), Var(v=0.1333, grad=0.0000), Var(v=-0.2557, grad=0.0000), Var(v=-0.0678, grad=0.0000), Var(v=0.1786, grad=0.0000), Var(v=-0.0824, grad=0.0000), Var(v=-0.0995, grad=0.0000), Var(v=-0.0103, grad=0.0000), Var(v=-0.0275, grad=0.0000), Var(v=-0.0375, grad=0.0000), Var(v=0.1337, grad=0.0000), Var(v=-0.0535, grad=0.0000), Var(v=0.0802, grad=0.0000), Var(v=-0.1159, grad=0.0000), Var(v=-0.0804, grad=0.0000), Var(v=-0.0580, grad=0.0000), Var(v=-0.1448, grad=0.0000), Var(v=0.0110, grad=0.0000), Var(v=0.1639, grad=0.0000), Var(v=-0.0498, grad=0.0000), Var(v=0.1139, grad=0.0000), Var(v=-0.0194, grad=0.0000), Var(v=-0.0358, grad=0.0000), Var(v=0.0035, grad=0.0000), Var(v=0.0584, grad=0.0000), Var(v=0.0052, grad=0.0000), Var(v=-0.2370, grad=0.0000), Var(v=0.2296, grad=0.0000), Var(v=0.0426, grad=0.0000), Var(v=-0.1402, grad=0.0000), Var(v=-0.1375, grad=0.0000), Var(v=0.0823, grad=0.0000), Var(v=-0.0594, grad=0.0000), Var(v=-0.1271, grad=0.0000), Var(v=-0.0090, grad=0.0000), Var(v=0.0291, grad=0.0000), Var(v=0.0410, grad=0.0000), Var(v=-0.0237, grad=0.0000), Var(v=-0.1335, grad=0.0000), Var(v=0.1442, grad=0.0000), Var(v=0.0152, grad=0.0000), Var(v=-0.0495, grad=0.0000), Var(v=0.0113, grad=0.0000)], [Var(v=-0.0230, grad=0.0000), Var(v=-0.0974, grad=0.0000), Var(v=0.0363, grad=0.0000), Var(v=-0.0911, grad=0.0000), Var(v=-0.0147, grad=0.0000), Var(v=0.0216, grad=0.0000), Var(v=-0.0704, grad=0.0000), Var(v=-0.1212, grad=0.0000), Var(v=-0.0641, grad=0.0000), Var(v=-0.1011, grad=0.0000), Var(v=-0.2170, grad=0.0000), Var(v=-0.0477, grad=0.0000), Var(v=-0.0769, grad=0.0000), Var(v=-0.1381, grad=0.0000), Var(v=0.0459, grad=0.0000), Var(v=-0.1407, grad=0.0000), Var(v=0.0625, grad=0.0000), Var(v=0.0976, grad=0.0000), Var(v=0.1410, grad=0.0000), Var(v=0.0494, grad=0.0000), Var(v=-0.0647, grad=0.0000), Var(v=0.0599, grad=0.0000), Var(v=-0.1083, grad=0.0000), Var(v=0.0549, grad=0.0000), Var(v=0.1428, grad=0.0000), Var(v=-0.0594, grad=0.0000), Var(v=-0.0057, grad=0.0000), Var(v=0.1400, grad=0.0000), Var(v=-0.0099, grad=0.0000), Var(v=0.0514, grad=0.0000), Var(v=-0.1530, grad=0.0000), Var(v=0.1448, grad=0.0000), Var(v=0.0045, grad=0.0000), Var(v=0.1521, grad=0.0000), Var(v=-0.0457, grad=0.0000), Var(v=0.0924, grad=0.0000), Var(v=0.0095, grad=0.0000), Var(v=-0.0012, grad=0.0000), Var(v=0.2218, grad=0.0000), Var(v=0.0399, grad=0.0000), Var(v=0.0732, grad=0.0000), Var(v=-0.1548, grad=0.0000), Var(v=-0.0953, grad=0.0000), Var(v=-0.1056, grad=0.0000), Var(v=-0.0484, grad=0.0000), Var(v=-0.1120, grad=0.0000), Var(v=-0.0173, grad=0.0000), Var(v=0.0895, grad=0.0000), Var(v=0.0286, grad=0.0000), Var(v=0.0044, grad=0.0000)], [Var(v=-0.0185, grad=0.0000), Var(v=0.0904, grad=0.0000), Var(v=0.1375, grad=0.0000), Var(v=-0.0327, grad=0.0000), Var(v=0.0297, grad=0.0000), Var(v=0.0270, grad=0.0000), Var(v=0.2811, grad=0.0000), Var(v=0.1674, grad=0.0000), Var(v=0.0065, grad=0.0000), Var(v=-0.0045, grad=0.0000), Var(v=-0.0988, grad=0.0000), Var(v=-0.3583, grad=0.0000), Var(v=0.0038, grad=0.0000), Var(v=0.0126, grad=0.0000), Var(v=-0.1645, grad=0.0000), Var(v=0.0446, grad=0.0000), Var(v=-0.0258, grad=0.0000), Var(v=-0.1593, grad=0.0000), Var(v=-0.1192, grad=0.0000), Var(v=-0.0009, grad=0.0000), Var(v=-0.0923, grad=0.0000), Var(v=0.2304, grad=0.0000), Var(v=0.1234, grad=0.0000), Var(v=0.0919, grad=0.0000), Var(v=0.0956, grad=0.0000), Var(v=-0.1140, grad=0.0000), Var(v=-0.1327, grad=0.0000), Var(v=-0.1356, grad=0.0000), Var(v=-0.0018, grad=0.0000), Var(v=0.1747, grad=0.0000), Var(v=0.1498, grad=0.0000), Var(v=-0.0247, grad=0.0000), Var(v=0.0976, grad=0.0000), Var(v=0.2138, grad=0.0000), Var(v=-0.0298, grad=0.0000), Var(v=0.0858, grad=0.0000), Var(v=-0.0518, grad=0.0000), Var(v=-0.0629, grad=0.0000), Var(v=-0.0960, grad=0.0000), Var(v=-0.0173, grad=0.0000), Var(v=0.0789, grad=0.0000), Var(v=-0.0126, grad=0.0000), Var(v=-0.0424, grad=0.0000), Var(v=-0.0332, grad=0.0000), Var(v=0.0017, grad=0.0000), Var(v=0.0388, grad=0.0000), Var(v=-0.1861, grad=0.0000), Var(v=0.0564, grad=0.0000), Var(v=-0.1230, grad=0.0000), Var(v=-0.0993, grad=0.0000)], [Var(v=-0.0680, grad=0.0000), Var(v=0.0613, grad=0.0000), Var(v=0.0589, grad=0.0000), Var(v=0.2238, grad=0.0000), Var(v=-0.0179, grad=0.0000), Var(v=-0.0599, grad=0.0000), Var(v=-0.0493, grad=0.0000), Var(v=0.0222, grad=0.0000), Var(v=-0.0795, grad=0.0000), Var(v=0.0636, grad=0.0000), Var(v=-0.0619, grad=0.0000), Var(v=-0.0031, grad=0.0000), Var(v=0.0873, grad=0.0000), Var(v=0.1240, grad=0.0000), Var(v=-0.0777, grad=0.0000), Var(v=0.0177, grad=0.0000), Var(v=0.0226, grad=0.0000), Var(v=-0.1396, grad=0.0000), Var(v=0.1780, grad=0.0000), Var(v=0.0495, grad=0.0000), Var(v=-0.0366, grad=0.0000), Var(v=0.0979, grad=0.0000), Var(v=0.0299, grad=0.0000), Var(v=0.0489, grad=0.0000), Var(v=-0.0806, grad=0.0000), Var(v=-0.1319, grad=0.0000), Var(v=0.1019, grad=0.0000), Var(v=0.0998, grad=0.0000), Var(v=0.0368, grad=0.0000), Var(v=0.1133, grad=0.0000), Var(v=0.0127, grad=0.0000), Var(v=0.1236, grad=0.0000), Var(v=-0.0862, grad=0.0000), Var(v=-0.0992, grad=0.0000), Var(v=0.1581, grad=0.0000), Var(v=-0.0984, grad=0.0000), Var(v=0.0457, grad=0.0000), Var(v=0.2303, grad=0.0000), Var(v=-0.0273, grad=0.0000), Var(v=0.0196, grad=0.0000), Var(v=0.0721, grad=0.0000), Var(v=-0.0724, grad=0.0000), Var(v=0.0981, grad=0.0000), Var(v=0.0664, grad=0.0000), Var(v=0.0816, grad=0.0000), Var(v=-0.0720, grad=0.0000), Var(v=-0.0270, grad=0.0000), Var(v=0.0303, grad=0.0000), Var(v=0.0388, grad=0.0000), Var(v=-0.1155, grad=0.0000)], [Var(v=0.1001, grad=0.0000), Var(v=-0.0586, grad=0.0000), Var(v=-0.0263, grad=0.0000), Var(v=0.0554, grad=0.0000), Var(v=-0.0463, grad=0.0000), Var(v=0.1039, grad=0.0000), Var(v=-0.0805, grad=0.0000), Var(v=-0.0836, grad=0.0000), Var(v=0.0150, grad=0.0000), Var(v=0.0194, grad=0.0000), Var(v=-0.0841, grad=0.0000), Var(v=0.2141, grad=0.0000), Var(v=-0.0417, grad=0.0000), Var(v=0.1512, grad=0.0000), Var(v=0.1732, grad=0.0000), Var(v=0.1146, grad=0.0000), Var(v=0.0235, grad=0.0000), Var(v=-0.0735, grad=0.0000), Var(v=-0.0125, grad=0.0000), Var(v=-0.1590, grad=0.0000), Var(v=-0.0597, grad=0.0000), Var(v=-0.0559, grad=0.0000), Var(v=0.0912, grad=0.0000), Var(v=0.1122, grad=0.0000), Var(v=-0.0250, grad=0.0000), Var(v=-0.0161, grad=0.0000), Var(v=0.1177, grad=0.0000), Var(v=0.1163, grad=0.0000), Var(v=0.1896, grad=0.0000), Var(v=-0.0351, grad=0.0000), Var(v=0.0085, grad=0.0000), Var(v=0.1746, grad=0.0000), Var(v=-0.0271, grad=0.0000), Var(v=-0.0848, grad=0.0000), Var(v=-0.0202, grad=0.0000), Var(v=-0.0666, grad=0.0000), Var(v=0.0525, grad=0.0000), Var(v=-0.0695, grad=0.0000), Var(v=0.1094, grad=0.0000), Var(v=0.0287, grad=0.0000), Var(v=-0.0192, grad=0.0000), Var(v=-0.0388, grad=0.0000), Var(v=-0.0459, grad=0.0000), Var(v=0.0071, grad=0.0000), Var(v=0.0091, grad=0.0000), Var(v=0.0014, grad=0.0000), Var(v=0.0843, grad=0.0000), Var(v=-0.2273, grad=0.0000), Var(v=0.0313, grad=0.0000), Var(v=0.0226, grad=0.0000)], [Var(v=-0.1682, grad=0.0000), Var(v=-0.1121, grad=0.0000), Var(v=-0.0142, grad=0.0000), Var(v=-0.0005, grad=0.0000), Var(v=-0.1236, grad=0.0000), Var(v=-0.1059, grad=0.0000), Var(v=-0.0699, grad=0.0000), Var(v=-0.1441, grad=0.0000), Var(v=0.0391, grad=0.0000), Var(v=-0.1460, grad=0.0000), Var(v=0.0377, grad=0.0000), Var(v=0.0293, grad=0.0000), Var(v=0.1150, grad=0.0000), Var(v=0.0328, grad=0.0000), Var(v=-0.1097, grad=0.0000), Var(v=-0.0012, grad=0.0000), Var(v=-0.1222, grad=0.0000), Var(v=-0.0782, grad=0.0000), Var(v=0.0535, grad=0.0000), Var(v=-0.0098, grad=0.0000), Var(v=0.0153, grad=0.0000), Var(v=0.0509, grad=0.0000), Var(v=-0.1471, grad=0.0000), Var(v=-0.0561, grad=0.0000), Var(v=0.1645, grad=0.0000), Var(v=0.0443, grad=0.0000), Var(v=0.1114, grad=0.0000), Var(v=-0.1016, grad=0.0000), Var(v=-0.0728, grad=0.0000), Var(v=0.0800, grad=0.0000), Var(v=0.0817, grad=0.0000), Var(v=0.0600, grad=0.0000), Var(v=-0.0198, grad=0.0000), Var(v=0.0451, grad=0.0000), Var(v=-0.0342, grad=0.0000), Var(v=-0.1045, grad=0.0000), Var(v=-0.0027, grad=0.0000), Var(v=0.0809, grad=0.0000), Var(v=0.0238, grad=0.0000), Var(v=0.0925, grad=0.0000), Var(v=0.0710, grad=0.0000), Var(v=0.0058, grad=0.0000), Var(v=0.0723, grad=0.0000), Var(v=0.0275, grad=0.0000), Var(v=-0.0573, grad=0.0000), Var(v=0.1107, grad=0.0000), Var(v=-0.0611, grad=0.0000), Var(v=0.0160, grad=0.0000), Var(v=0.1622, grad=0.0000), Var(v=-0.0427, grad=0.0000)], [Var(v=0.0430, grad=0.0000), Var(v=-0.0656, grad=0.0000), Var(v=0.0600, grad=0.0000), Var(v=-0.1209, grad=0.0000), Var(v=0.0129, grad=0.0000), Var(v=-0.0592, grad=0.0000), Var(v=0.0400, grad=0.0000), Var(v=-0.0171, grad=0.0000), Var(v=-0.1256, grad=0.0000), Var(v=-0.0797, grad=0.0000), Var(v=0.0057, grad=0.0000), Var(v=0.0897, grad=0.0000), Var(v=-0.0859, grad=0.0000), Var(v=0.1830, grad=0.0000), Var(v=-0.1157, grad=0.0000), Var(v=0.1066, grad=0.0000), Var(v=-0.0889, grad=0.0000), Var(v=0.0805, grad=0.0000), Var(v=-0.0596, grad=0.0000), Var(v=0.0495, grad=0.0000), Var(v=-0.1089, grad=0.0000), Var(v=0.0261, grad=0.0000), Var(v=-0.1168, grad=0.0000), Var(v=0.0589, grad=0.0000), Var(v=-0.0218, grad=0.0000), Var(v=0.0482, grad=0.0000), Var(v=-0.1228, grad=0.0000), Var(v=-0.0760, grad=0.0000), Var(v=0.0733, grad=0.0000), Var(v=-0.0073, grad=0.0000), Var(v=0.0767, grad=0.0000), Var(v=-0.0488, grad=0.0000), Var(v=0.0216, grad=0.0000), Var(v=-0.1021, grad=0.0000), Var(v=-0.1707, grad=0.0000), Var(v=0.1442, grad=0.0000), Var(v=-0.1705, grad=0.0000), Var(v=-0.0926, grad=0.0000), Var(v=0.1652, grad=0.0000), Var(v=-0.1452, grad=0.0000), Var(v=0.0562, grad=0.0000), Var(v=0.0124, grad=0.0000), Var(v=-0.0935, grad=0.0000), Var(v=0.0673, grad=0.0000), Var(v=-0.0336, grad=0.0000), Var(v=-0.0298, grad=0.0000), Var(v=0.0311, grad=0.0000), Var(v=0.0384, grad=0.0000), Var(v=-0.0260, grad=0.0000), Var(v=-0.0669, grad=0.0000)], [Var(v=0.2684, grad=0.0000), Var(v=0.1023, grad=0.0000), Var(v=0.0080, grad=0.0000), Var(v=-0.1075, grad=0.0000), Var(v=0.1242, grad=0.0000), Var(v=-0.0436, grad=0.0000), Var(v=-0.0564, grad=0.0000), Var(v=-0.1048, grad=0.0000), Var(v=0.2006, grad=0.0000), Var(v=-0.1082, grad=0.0000), Var(v=-0.0278, grad=0.0000), Var(v=0.1813, grad=0.0000), Var(v=-0.0246, grad=0.0000), Var(v=0.0153, grad=0.0000), Var(v=0.2094, grad=0.0000), Var(v=0.0068, grad=0.0000), Var(v=0.0519, grad=0.0000), Var(v=0.0068, grad=0.0000), Var(v=0.1407, grad=0.0000), Var(v=-0.2884, grad=0.0000), Var(v=0.2353, grad=0.0000), Var(v=0.0601, grad=0.0000), Var(v=0.1291, grad=0.0000), Var(v=-0.0513, grad=0.0000), Var(v=-0.1170, grad=0.0000), Var(v=0.0141, grad=0.0000), Var(v=0.0304, grad=0.0000), Var(v=0.0139, grad=0.0000), Var(v=-0.0886, grad=0.0000), Var(v=0.0091, grad=0.0000), Var(v=0.0241, grad=0.0000), Var(v=-0.0375, grad=0.0000), Var(v=-0.0049, grad=0.0000), Var(v=-0.0360, grad=0.0000), Var(v=-0.0639, grad=0.0000), Var(v=0.2345, grad=0.0000), Var(v=0.0406, grad=0.0000), Var(v=-0.0911, grad=0.0000), Var(v=-0.0315, grad=0.0000), Var(v=0.0355, grad=0.0000), Var(v=0.0027, grad=0.0000), Var(v=0.0334, grad=0.0000), Var(v=-0.0117, grad=0.0000), Var(v=-0.1197, grad=0.0000), Var(v=0.1534, grad=0.0000), Var(v=-0.0844, grad=0.0000), Var(v=0.1814, grad=0.0000), Var(v=-0.0674, grad=0.0000), Var(v=-0.0882, grad=0.0000), Var(v=0.0766, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
      "Layer 2 \n",
      " Weights: [[Var(v=0.1259, grad=0.0000)], [Var(v=0.1001, grad=0.0000)], [Var(v=0.0469, grad=0.0000)], [Var(v=0.0235, grad=0.0000)], [Var(v=-0.1436, grad=0.0000)], [Var(v=-0.0939, grad=0.0000)], [Var(v=-0.0325, grad=0.0000)], [Var(v=0.0222, grad=0.0000)], [Var(v=-0.0984, grad=0.0000)], [Var(v=0.0655, grad=0.0000)], [Var(v=0.0555, grad=0.0000)], [Var(v=0.0674, grad=0.0000)], [Var(v=-0.0067, grad=0.0000)], [Var(v=-0.0809, grad=0.0000)], [Var(v=0.0027, grad=0.0000)], [Var(v=-0.1017, grad=0.0000)], [Var(v=0.1347, grad=0.0000)], [Var(v=0.1161, grad=0.0000)], [Var(v=-0.0783, grad=0.0000)], [Var(v=-0.0714, grad=0.0000)], [Var(v=0.0044, grad=0.0000)], [Var(v=0.0474, grad=0.0000)], [Var(v=0.0914, grad=0.0000)], [Var(v=-0.0201, grad=0.0000)], [Var(v=-0.0661, grad=0.0000)], [Var(v=0.0872, grad=0.0000)], [Var(v=-0.0453, grad=0.0000)], [Var(v=0.0057, grad=0.0000)], [Var(v=0.1406, grad=0.0000)], [Var(v=-0.0051, grad=0.0000)], [Var(v=-0.1653, grad=0.0000)], [Var(v=0.1622, grad=0.0000)], [Var(v=-0.0684, grad=0.0000)], [Var(v=0.0205, grad=0.0000)], [Var(v=0.0438, grad=0.0000)], [Var(v=-0.1548, grad=0.0000)], [Var(v=-0.1727, grad=0.0000)], [Var(v=0.0205, grad=0.0000)], [Var(v=-0.0046, grad=0.0000)], [Var(v=-0.0635, grad=0.0000)], [Var(v=0.0127, grad=0.0000)], [Var(v=-0.1018, grad=0.0000)], [Var(v=0.0273, grad=0.0000)], [Var(v=-0.0590, grad=0.0000)], [Var(v=-0.1019, grad=0.0000)], [Var(v=0.0644, grad=0.0000)], [Var(v=-0.0768, grad=0.0000)], [Var(v=-0.0693, grad=0.0000)], [Var(v=-0.2110, grad=0.0000)], [Var(v=-0.0403, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Network before update:')\n",
    "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \n",
    "\n",
    "def parameters(network):\n",
    "  params = []\n",
    "  for layer in range(len(network)):\n",
    "    params += network[layer].parameters()\n",
    "  return params\n",
    "\n",
    "def update_parameters(params, learning_rate=0.01):\n",
    "  for p in params:\n",
    "    p.v -= learning_rate*p.grad\n",
    "\n",
    "def zero_gradients(params):\n",
    "  for p in params:\n",
    "    p.grad = 0.0\n",
    "\n",
    "update_parameters(parameters(NN))\n",
    "\n",
    "print('\\nNetwork after update:')\n",
    "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] \n",
    "\n",
    "zero_gradients(parameters(NN))\n",
    "\n",
    "print('\\nNetwork after zeroing gradients:')\n",
    "[print('Layer', i, '\\n', NN[i]) for i in range(len(NN))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "woWYpdw6FtIO"
   },
   "outputs": [],
   "source": [
    "# Initialize an arbitrary neural network\n",
    "NN = [\n",
    "    DenseLayer(1, 8, lambda x: x.relu()),\n",
    "    DenseLayer(8, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "# Recommended hyper-parameters for 3-D: \n",
    "#NN = [\n",
    "#    DenseLayer(3, 16, lambda x: x.relu()),\n",
    "#    DenseLayer(16, 1, lambda x: x.identity())\n",
    "#]\n",
    "\n",
    "\n",
    "### Notice that, when we switch from tanh to relu activation, we decrease the learning rate. This is due the stability of the gradients \n",
    "## of the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "mdqaqYBVFtIR"
   },
   "outputs": [],
   "source": [
    "# Initialize training hyperparameters\n",
    "EPOCHS = 200\n",
    "LEARN_R = 2e-3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kfg76GMFtIW",
    "outputId": "e30cf68a-31f2-42b4-cc5e-860c297c0f04",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 ( 0.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      "  10 ( 5.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      "  20 (10.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      "  30 (15.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      "  40 (20.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      "  50 (25.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      "  60 (30.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      "  70 (35.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      "  80 (40.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      "  90 (45.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      " 100 (50.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      " 110 (55.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      " 120 (60.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      " 130 (65.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      " 140 (70.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      " 150 (75.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      " 160 (80.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      " 170 (85.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      " 180 (90.00%) Train loss: 105.000 \t Validation loss: 107.757\n",
      " 190 (95.00%) Train loss: 105.000 \t Validation loss: 107.757\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "     \n",
    "    # Forward pass and loss computation\n",
    "    Loss = squared_loss(y_train, forward(x_train, NN))\n",
    "\n",
    "    # Backward pass\n",
    "    Loss.backward()\n",
    "    \n",
    "    # gradient descent update\n",
    "    update_parameters(parameters(NN), LEARN_R)\n",
    "    zero_gradients(parameters(NN))\n",
    "    \n",
    "    # Training loss\n",
    "    train_loss.append(Loss.v)\n",
    "    \n",
    "    # Validation\n",
    "    Loss_validation = squared_loss(y_validation, forward(x_validation, NN))\n",
    "    val_loss.append(Loss_validation.v)\n",
    "    \n",
    "    if e%10==0:\n",
    "        print(\"{:4d}\".format(e),\n",
    "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
    "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "VetyRWFwFtIY",
    "outputId": "344e490d-6d7d-455a-fa6f-88dd11eb957e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(train_loss)), train_loss);\n",
    "plt.plot(range(len(val_loss)), val_loss);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OgmIrM9FtIb"
   },
   "source": [
    "# Testing\n",
    "\n",
    "We have kept the calculation of the test error separate in order to emphasize that you should not use the test set in optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "HmNi7S-vFtIc"
   },
   "outputs": [],
   "source": [
    "output_test = forward(x_test, NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "7mmJOTSEFtIf",
    "outputId": "e3264095-cefe-4aee-893d-bf152438e332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  100.013\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_test_np = Var_to_nparray(y_test)\n",
    "plt.scatter(y_test_np, Var_to_nparray(output_test));\n",
    "plt.plot([np.min(y_test_np), np.max(y_test_np)], [np.min(y_test_np), np.max(y_test_np)], color='k');\n",
    "plt.xlabel(\"y\");\n",
    "plt.ylabel(\"$\\hat{y}$\");\n",
    "plt.title(\"Model prediction vs real in the test set, the close to the line the better\")\n",
    "plt.grid(True);\n",
    "plt.axis('equal');\n",
    "plt.tight_layout();\n",
    "\n",
    "Loss_test = squared_loss(y_test, forward(x_test, NN))\n",
    "\n",
    "print(\"Test loss:  {:4.3f}\".format(Loss_test.v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "ODi0WlmQFtIh",
    "outputId": "d1ab874f-0717-4987-87bf-1f0c7c8e7148"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test_np = Var_to_nparray(x_test)\n",
    "x_train_np = Var_to_nparray(x_train)\n",
    "y_train_np = Var_to_nparray(y_train)\n",
    "if D1:\n",
    "    plt.scatter(x_train_np, y_train_np, label=\"train data\");\n",
    "    plt.scatter(x_test_np, Var_to_nparray(output_test), label=\"test prediction\");\n",
    "    plt.scatter(x_test_np, y_test_np, label=\"test data\");\n",
    "    plt.legend();\n",
    "    plt.xlabel(\"x\");\n",
    "    plt.ylabel(\"y\");\n",
    "else:\n",
    "    plt.scatter(x_train_np[:,1], y_train, label=\"train data\");\n",
    "    plt.scatter(x_test_np[:,1], Var_to_nparray(output_test), label=\"test data prediction\");\n",
    "    plt.scatter(x_test_np[:,1], y_test_np, label=\"test data\");\n",
    "    plt.legend();\n",
    "    plt.xlabel(\"x\");\n",
    "    plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTBAmjsAFtIk"
   },
   "source": [
    "## Exercise k) Show overfitting, underfitting and just right fitting\n",
    "\n",
    "Vary the architecture and other things to show clear signs of overfitting (=training loss significantly lower than test loss) and underfitting (=not fitting enoung to training data so that test performance is also hurt).\n",
    "\n",
    "See also if you can get a good compromise which leads to a low validation loss. \n",
    "\n",
    "For this problem do you see any big difference between validation and test loss? The answer here will probably be no. Discuss cases where it is important to keep the two separate.\n",
    "\n",
    "_Insert written answer here._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "tQZCn2dxFtIl"
   },
   "outputs": [],
   "source": [
    "# Insert your code for getting overfitting, underfitting and just right fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYPZP-eTFtIo"
   },
   "source": [
    "# Next steps - classification\n",
    "\n",
    "It is straight forward to extend what we have done to classification. \n",
    "\n",
    "For numerical stability it is better to make softmax and cross-entropy as one function so we write the cross entropy loss as a function of the logits we talked about last week. \n",
    "\n",
    "Next week we will see how to perform classification in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "U4057_ljNvWB",
    "p_8n_SKnIW2F",
    "oLrGJytZFtGm",
    "jpIZPBpNI0pO",
    "_79HOAXrFtHK",
    "mqeyab9qFtGs",
    "-XyXBD37FtHk",
    "SrwSJ2UWFtHu",
    "zTBAmjsAFtIk",
    "qsVPul3QFtIo",
    "APqhJv3tta1O"
   ],
   "name": "2.1-EXE-FNN-AutoDif-Nanograd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
